#+PROPERTY: header-args:lisp :results replace :session
#+PROPERTY: header-args:python :results none :session test :exports none

#+COLUMNS: %25ITEM %TAGS %PRIORITY %TODO

* LaTeX                                                            :noheading:

#+BEGIN_SRC emacs-lisp :exports none
(setenv "PYTHONPATH" (concat (getenv "PYTHONPATH") ":./code/spencer"))
(rainbow-delimiters-mode -1)
(color-identifiers-mode -1)
#+END_SRC

#+TITLE: Online activity recognition through kernel methods
#+AUTHOR: Evgeni Pavlidis

#+LaTeX_CLASS: scrbook
#+LaTeX_CLASS_OPTIONS: [11pt,a4paper,bibtotoc,idxtotoc,headsepline,footsepline,footexclude,BCOR12mm,DIV13]
#+LaTeX_CMD: xelatex

# --- Packages
#
#+LaTeX_HEADER: \usepackage[top=45mm, bottom=50mm]{geometry}
#+LaTeX_HEADER: \usepackage{pdfsync}
#+LaTeX_HEADER: \usepackage{scrpage2}

#+LaTeX_HEADER: \usepackage{hyperref}


#+LaTeX_HEADER: \usepackage{palatino}
#+LaTeX_HEADER: \usepackage{pifont}
#+LaTeX_HEADER: \usepackage{rotating}
#+LaTeX_HEADER: \usepackage{float}
#+LaTeX_HEADER: \usepackage[utf8]{inputenc}
#+LaTeX_HEADER: \usepackage{marvosym}

#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amsfonts}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{bm}
#+LaTeX_HEADER: \usepackage{textcomp}

#+LaTeX_HEADER: \usepackage{makeidx}
#+LaTeX_HEADER: \usepackage{subfigure}
#+LaTex_HEADER: \usepackage{graphicx}

#+LaTeX_HEADER: \usepackage{todonotes}
#+LaTeX_HEADER: \usepackage{setspace}


#+LaTeX_HEADER: \usepackage{titlesec}
#+LaTeX_HEADER: \usepackage{emptypage}
#+LaTeX_HEADER: \usepackage{styles/tumlogo}


# --- Options
#
#+LaTeX_HEADER: \pagestyle{scrheadings}



# --- TITLE ---
#
#+LaTeX_HEADER: \let\OldMaketitle\maketitle
#+LaTeX_HEADER: \renewcommand{\maketitle}{
#+LaTeX_HEADER: \pagenumbering{roman} 
#+LaTeX_HEADER:
#+LaTeX_HEADER: }


#+begin_latex
#+end_latex


#+begin_latex
\include{components/info}
\include{components/cover}
\include{components/titlepage}

\include{components/abstract}
\include{components/abstract_german}
\include{components/disclaimer}
#+end_latex






# --- Table of Contents
# 
#+OPTIONS: toc:nil   
#+TOC: headlines 2

# --- Bibliography
#
#+BIBLIOGRAPHY: bibliography plain limit:t
#+STYLE: &lt;link rel="stylesheet" type="text/css" href="css/org.c


 
* Introduction

#+begin_latex

\newcommand{\TODO}[1]{\todo[color=red]{#1}}
\pagenumbering{arabic} 


\setcounter{secnumdepth}{2}

#+end_latex


\TODO{cite:software packages and tools used}
\TODO{cite:datasets (mocap, daily activities, ms activities)}
\TODO{Check bibliography style and data!!!}
\TODO{define simple variables mathematically eg. feature sequence etc.}


** Motivation
Activity recognition is a big research field in machine learning and robotics. Being able to infer what human actors are doing helps in many practical robotic tasks. An example is doing short-time prediction for collision-avoidance. Moreover in social robotics it is crucial to know what humans are doing when reasoning about the current state of the robot's environment.

To do activity recognition the human pose has to be inferred for each frame.  
With the advent and further development of RGBD (color and depth) sensors it is now possible to perform skeletal tracking of persons. This allows us to decouple pose estimation and activity learning, which make the problem a bit easier.

For real robotic tasks it is very important that the activity recognition is online i.e. runs in real-time. 

Advantages of skeletal features are that no person sensitive information has to be processed by the learning algorithm. In the context of the SPENCER project this is an important prerequisite. Another advantage is also that the skeletal features are very informative for activity recognition. Also beginning to concept an algorithm that builds on top of robust pose estimation reduces the complexity, as we can fully ignore the pose estimation problem.


*** The SPENCER project
With the modern technologies (ms Kinect SDK, primesense ...) it is possible to decouple skeleton tracking and learning




** Problem statement

\todo{make a distinction between action and activity}
\todo{make a distinction between online recognition and online learning !!! maybe change online to real-time}

The biggest challenge in activity recognition is classification of time series data. In contrast to the simpler sample model, here we have to classify sequences. Therefore appropriate models have to be implemented that take the dynamics into account.

On top of that the problem is ill-posed. 

Also for online activity recognition we have to classify incomplete data consisting of subsequences. This makes the problem more challenging and reduces the pool of methods that can be used for the classification.

We identify three requirements for a practical activity recognition system:
1. Online recognition\\
   The algorithm has to be able to do real-time classification as the sequence is progressing.
2. Classification of incomplete sequences\\
   This follows directly from the first requirement. There should be no assumptions about the activities regarding completeness, length or periodicity.
3. Novelty detection\\
   The algorithm should be able to recognize unobserved activities. 

Optimally it should be possible to also recognize when an unknown activity started and finished. This way automatic segmentation will be possible and will considerably reduce the supervised learning time. \todo{elaborate on this} In combination with active learning this will greatly reduce training time for practical applications.

Nice to have... online learning??

*** Online learning of human activities
Use Gaussian processes to learn new activities in real time
*** Evaluate Gaussian processes against different ml algorithms for activity recognition
Evaluate the performance of GPs in relation to the other solutions

*** Scope
** Prerequisites and notation
We assume a basic understanding in /Linear Algebra/ and /Probability theory/ . Although a high-level overview on machine learning is given in chapter 2 deeper knowledge in this field will help understand the /Related Works/ chapter better.

*** Mathematical notation
- Matrices uppercase
- Vectors lowercase bold
- Constants lowercase
- Parameters lowercase greek letters
** Outline
- Introduction ::
   This chapter introduced the topic of this work. The motivation and the scope is explained.
- Background ::
   The second chapter summarizes some basic concepts and models that are prerequisites for our approach. It begins with an overview of machine learning and introduces the multivariate Gaussian distribution. Then an emphasis is led on Gaussian Process Regression and Gaussian Process - Latent Variable Models, which is an unsupervised learning method for dimensionality reduction. Last the Dynamic Time Warping algorithm, which is used for sequence alignment, is explained.
- Related Work :: The third chapter gives an overview of methods used in similar approaches and then analyses strength and weaknesses of these methods in regards to online activity recognition.
- Approach :: The fourth chapter presents two approaches to online activity recognition and their implementations. The first one is an implementation of "Discriminative Sequence Back-constrained {GP}-{LVM} for {MOCAP} based Action Recognition}" ebib:_discriminative_2013. The second one is a novel approach which learns a dense motion flow field in latent space through Gaussian Process Regression.
- Evaluation :: In the fifth chapter the two approaches are being evaluated and discussed. 
- Results and Outlook :: The last chapter summarizes the results of the two approaches and gives a brief outlook of future improvements.

* Background
This chapter introduces some basic concepts needed to understand the proposed approaches. First a high-level overview is given on machine learning and its terminology. Then the Kernel function is explained along with the /Support Vector Machine/ - a kernelized learning method. Following is an explanation of /Gaussian Processes/, their different interpretations and properties. After that the /Gaussian Process - Latent Variable Model/ is being introduced along with some extensions for learning a backward mapping and optimizing it for discrimination in the case of multiple classes. Last two /Sequence similarities measures/ are presented which are used in our implementations.
 
** Machine Learning
Machine Learning is a discipline where one makes inference on real world data. Data consists of different samples

*** Supervised learning
Supervised learning is the task of classification or regression when the data is labeled i.e. we have the ground truth of every sample.
The algorithm then takes the labeled samples (and maybe some confidence values) and infers the model parameters (or hyperparameters) accordingly.

There are two distinct cases in supervised learning:

**** *Classification*

Classification is the task of learning which category a sample belongs to. A prominent example is Spam filtering. By taking a large number of emails which are labeled either as spam or as ham (regular email), the algorithm deduces a model which can classify unknown samples into these two categories.

**** *Regression*

Regression is a terminus in machine learning and means function approximation. Here the domain of the sample's label is continuous. 
An example would be ...


In most cases we search for a good model that explains the data we have. Parametric models, for example, have a pre-defined model which is parametrized. 
When searching for an appropriate model it is also important that we try to capture the underlying relationship without compromising the generalization property, which is the ability of the model to correctly predict unseen samples. The case that an algorithm learns the relationship of the data that is used to train the model (training data) but poorly predicts new samples is called overfitting.  





Very often the parameter search is done by maximizing the probability of the data given the model parameters. 

$$ \operatorname{arg\,max}_{\bm{\theta}} p(\bm{X} | \bm{\theta}) = \operatorname{arg\,max}_{\bm{\theta}} \frac{p(\bm{\theta}|\bm{X}) p(\bm{X})}{p(\bm{\theta})} $$

where $\theta$ are the model parameters and $X$ is the data.

*** Unsupervised learning
In contrast to supervised learning in unsupervised learning we have no labeled data i.e. there is no supervisor giving each sample a category (classification) or a value (regression). In this case we can only derive properties of the generation process. Therefore we try to detect patterns in the unlabeled data. These pattern may be clusters of similarity or a lower dimensional generative manifold from which the samples are generated. The last one is called /Dimensionality Reduction/ which will be also a subject in this work. ebib:bishop_pattern_2006 

*K-means algorithm*

An example of an unsupervised learning method for finding a given number of clusters $k$ in given data is the /k-means/ method. The idea is that we first determine the number of clusters and choose $k$ points randomly in the space, which represent a guess of the cluster means (center of mass). After that we try to move these points, such that they align with the real data's $k$ centers of mass. This is done by iterating between two steps:

1. Assign each point $x$ to the closest centroid (cluster mean)
2. Find new centroids by computing the mean of all assigned points for each cluster $k$

Doing so it is guaranteed that the algorithm will converge, although it could be in a local minimum. 

ebib:bishop_pattern_2006

*** Generative models
Generative methods model the underlying process which generates the data. In Bayesian terms we model the likelihood and the . Thus more data is needed to find an appropriate model. On the other side the model is very flexible and many attributes have a natural interpretation. An example of this is \todo{generative model example}

*** Discriminative models
A discriminative model is only concerned with modeling the actual posterior. This way fewer samples are needed to find an appropriate model. On the other hand by not taking the prior into account the model's ability to generalize unseen data is worse. For this reason discriminative methods are more susceptible to overfitting.

*** Online learning
Algorithms which can be gradually optimized towards a good solution using streaming batches of samples are considered to do online learning. In contrast to online learning online recognition means that the algorithm works in real-time and fast recognition is possible. 

*** Active learning
Very often the bottleneck of powerful supervised learning techniques is that they rely on a large number of correctly labeled data. Since labeling has to be performed by a human it is very difficult and costly to label large amount of data. By identifying more important samples by their information ability of selecting a good model, it is possible to achieve good results with fewer samples. Letting the algorithm select such samples and query only their labels from a human, who is now actively participating in the learning loop, is called active learning. 

Active learning is in practice a convenient way to acquire new informative samples without letting someone go over a huge amount of data to label.

** Kernel methods
Many machine learning algorithms work not with the features directly but instead use only the dot product between features. The dot product between two vectors can be seen as a measure of similarity. 

*** A space defined by sample similarity

Suppose we have $n$ sample points $\bm{x_i}$ of dimensionality $d$: $\bm{x_i} \in \mathcal{R}^d$. When extracting features we try to capture the most characteristic properties of the data for each sample. Let us say that we want to extract $m$ features. Then we have a vector $\bm{z_i} \in \mathcal{R}^m$ which represents each sample. This means that learning is done in a feature space of dimensionality $m$. Another space, where we can reason about the data is a similarity space. Suppose we have a function $k(\bm{x},\bm{y})$ which measures the similarity between point $\bm{x}$ and point $\bm{y}$, then we can define a vector $\bm{s}$ of similarities for a new point $\bm{x_{new}$ by computing the similarity of this point with every other sample: $s_i = k(\bm{x_{new}, \bm{x_i})$. With this we have a vector with $n$ elements each telling us how close the new point is to every other point. If we want to solve a classification problem, for example, it is much easier to create a decision plane inside the $n$ dimensional space instead of an lower $m$ dimensional feature space. 

This similarity measure is also called a /kernel function/.
We can also define some properties of the kernel function resulting of the informal introduction as a similarity measure.

*** The Kernel trick
A kernel defines a similarity measure between two points $\bm{x}$ and $\bm{y}$. The kernel function can be defined as the dot product between two feature vectors. 
$$ k(\bm{x},\bm{y}) = \phi(\bm{x})^T \phi(\bm{y}) $$
where $\phi(\bm{x})$ is a mapping from the input space (raw data) to a feature space.

If a machine learning algorithm is formulated only in terms of the dot product of two feature vectors, it this term can be exchanged with a kernel. As the kernel defines the feature space, we can work in feature space which are high- and even infinite-dimensional. 
This is called the kernel trick.
*** The Radial Basis Function

*** Support Vector Machines
Suppose we have data which is linearly separable. If we have only two features we can draw all samples in a 2D plot. This is shown in Figure [[fig:support-vectors]]. In this case the /best/ line that can separate both classes should be as far apart from all samples as possible. This line can be defined by the samples that are nearest to it. These samples are called support vectors as they are sufficient to span the boundary. For this reason /SVM/ is also called a sparse method as one only needs the support vectors to define the classification boundary. For higher dimensional feature spaces the same idea holds, but instead of having a line we have a plane (a hyperplane) which dissects the space in two parts. As the /SVM/ models the boundary between each class without considering any generative process it is a discriminative model.


#+Caption: SVM decision boundary (red) between two classes (cross, circle). The support vectors are indicated in green.
#+Label: fig:support-vectors
#+ATTR_LATEX: :width 10cm
[[file:figures/support-vectors.eps]]



The assumption that the data is linearly separable can be relaxed in two ways:

Instead of finding a boundary in the feature space we can use the kernel trick to project the data into a kernel space. This way the data may not be linearly separable in the feature space, but instead could be linearly separated in some kernel space. If we take the /Radial Basis Funciton/ for example the kernel space has infinite dimensions and thus the data can be linearly separated. 

We can also allow for a small subset of samples to cross the boundary without compromising its discriminative properties. This is called the /soft-margin SVM/.

The theory behind /SVM/ and the fact that the support vectors can be found by optimizing a convex function make this method a very robust way to do classification. For this reason there are multiple implementations of /SVMs/ which are very popular and are used very often in practical applications.

** Gaussian Processes
Consider the multivariate Gaussian distribution above. If we want to model the distribution of a discrete function defined over a finite interval,
we can treat each element of the vector $\bm{x}$ as a point of the function. Thus we can view the multivariate Gaussian distribution as a probability density function over the function space. Letting the dimensionality $d$ go to infinity (the distance between each point goes to zero) we can model continuous functions.

In this case the mean is a point in function space, thus a function $E[\bm{x}] = f(x)$. And because of the fact that we now have infinite dimensions the covariance can be seen as an "/infinite/ matrix/", thus a function of two elements: $Cov(x,y)$. This can be also seen as a kernel as discussed in [[Kernel Methods]].Therefore it can be seen as a Gaussian distribution over function space.ebib:rasmussen_Gaussian_2006

The marginalization property is what makes Gaussian Processes feasible as it lets us compute likelihoods with a finite part of the covariance function -- which can be seen as a covariance matrix. 

A Gaussian process can be also seen as the bayesean posterior consisting of the product of the (Gaussian) functional prior and the observed samples. Another view is a kernelized regression with infinite parameters. ebib:rasmussen_Gaussian_2006

A Gaussian process is a non-parametric model and is governed by the hyperparameters of the used kernel. This also means that the model is less prune to overfitting which is an important property as it not needed to perform cross validation.

*** The Gaussian distribution
**** Univariate Gaussian distribution
In the one dimensional case the Gaussian distribution is well known and understood. Moreover many processes in nature can be modeled with this distribution and for this reason it is also called the Normal distribution. The probability of an event is very high on a certain "point" (its meain value $\mu$) and it drops quickly on each side with the standard deviation $\sigma$.

$$ \mathcal{N}(\mu, \sigma^2) = \frac{1}{\sigma  \sqrt{2 \pi}}e^{-\frac{x-\mu}{2 \sigma^2}} $$

#+begin_src python :results file :session :results replace :exports results
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.mlab as mlab

mean = 0
variance = 1
sigma = np.sqrt(variance)
x = np.linspace(-5,5,100)
plt.figure(figsize=(10,5))
plt.plot(x,mlab.normpdf(x,mean,sigma))
plt.ylim(-0.1,0.5)
plt.xlim(-5,5)
plt.savefig('figures/univariate-Gaussian.eps')
'figures/univariate-Gaussian.eps'
#+end_src

#+Caption: The univariate Gaussian distribution with mean $\mu = 0$ and variance $\sigma^2 = 1$
#+Label: fig:univariate-gaussian
#+ATTR_LATEX: :width 10cm
#+RESULTS:
[[file:figures/univariate-gaussian.eps]]

As you can see in Figure [[fig:univariate-gaussian]] 



One disadvantage of this distribution which we can see from the above formula is that it can model only one hypothesis. This is also the case for the Gaussian distributions of multiple (multivariat Gaussian distribution) and infinite (Gaussian process) dimensions.

**** Multivariate Gaussian distribution
The multivariat Gaussian distribution is the generalization of the Gaussian distribution in higher dimensions.

$$ \mathcal{N}(\bm{\mu}, \bm{\Sigma}) =  \frac{1}{  \sqrt{(2 \pi)^d |\bm{\Sigma}|}}
e^{-\frac{1}{2} (\bm{x} - \bm{\mu})^T \bm{\Sigma}^{-1}  (\bm{x} - \bm{\mu})} $$

The two parameters of the distribution are:
- mean :: $\bm{\mu} = E[x]$ Representing the most probable vector
- covariance :: $\bm{\Sigma}$ Representing the mutual variance for each pair of the elements of the random vector: $\bm{\Sigma}_{ij} = Cov[x_i, x_j]$

The exponent is mahalanobis distance, which measures the distance of a point to the ellipsoid defined by the covariance matrix.\todo{cite}

**** Properties of Gaussian distributions
\todo{Write about total probability and such}
Aside for being an appropriate model for many processes occurring in nature, Gaussian distributions are also very nice to work with. The marginal and conditional of two Gaussian distributions are also Gaussian. 

One reason GPs are straightforward and work is the math behind them. It is just linear algebra operations.

Linear maps for Gaussian distributions:

Product of two multivariate Gaussian distributions:

$$ \mathcal{N}(\bm{x}; \bm{\mu_x}, \bm{\Sigma_x} ) \mathcal{N}(\bm{y}; \bm{\mu_y}, \bm{\Sigma_y} ) =  
\frac{1}{  \sqrt{(2 \pi)^d |\bm{\Sigma}|}} e^{-\frac{1}{2} (\bm{x} - \bm{\mu_x})^T \bm{\Sigma}^{-1}  (\bm{x} - \bm{\mu})} 
\frac{1}{  \sqrt{(2 \pi)^d |\bm{\Sigma}|}} e^{-\frac{1}{2} (\bm{x} - \bm{\mu_y})^T \bm{\Sigma}^{-1}  (\bm{x} - \bm{\mu_y})} $$


Marginal of a multivariate Gaussian:

Conditional of a multivariate Gaussian: 

\todo{cite Gaussian Winter School slides Philipp Hennig Gaussian Process Summer School 2014 }

*** Kernels
The most used kernel when using Gaussian Process is the /Radial Basis Function (RBF)/.

**** Effects of the hyper-parameters
*** Regression
#+begin_src python :results file :session :results replace :exports results

#plt.savefig('figures/gp-regression.eps')
#'figures/gp-regression.eps'
#+end_src

#+Caption: Regression example: 
#+Label: fig:regression
#+ATTR_LATEX: :width 10cm
#+RESULTS:



With /Gaussian Processes/ we don't learn a model, but instead we have a probability over infinitely many models with the mean being the most probable one. 

*** Learning
In the case of a GP the learning phase is different than in parametric models, where the model parameters are inferred from the data.
Training in the case of GPs means finding good hyperparameter for the kernel, by reducing the log-likelihood by variational optimization  (data fit term + cov. regularizer)

In contrast to parametric models Gaussian processes are less prune to overfitting because of the covariance regularizer term.


$$ E(\theta) = \frac{1}{2}\log({K}) - \frac{y^T K^{-1} y}{2} $$

We see that we have to invert the covariance matrix, which is of dimensions $n \times n$. Therefore this operation has a runtime complexity of $\mathcal{O}(n) = n^3$ which is also the bottleneck of the whole algorithm and a serious drawback of Gaussian Processes.

*** Classification
Classifying with GPs is a little more involved, because of the
discriminative function and the fact that the likelihood \todo{explain
problems of GP classification right} is not a Gaussian. For this
reason different models exist which try to approximate this
likelihood.

*** Advantages
**** non parametric
When using a parametric model one has to make sure that the chosen model is sufficiently complex to fit the data but at the same time is not too complex that it will overfitt the training data. This is a very hard task and is in most cases done through cross-validation of the model with an independent validation set. As discussed above GPs are less prune to overfitting and therefore we do not need to reduce the training data to create a validation set.

**** probabilistic
Being a model which has a Bayesian interpretation GP
The hyperparameters can be interpreted. The lenghtscale controls how much neighboring points contribute to the covariance of the function.

**** generative


**** nice for Bayesian
**** linear algebra operations (marginals and conditionals)
*** Disadvantages

**** susceptible to outliers
One big problem of the Gaussian distribution is that it has the assumption that the noise is Gaussian. When this assumption does not hold and we have several an outlier it either shift the mean un-proportionally to itself or raise the variance. Both cases are
The student-t distribution, for exmaple, is robust against outliers but is much harder to deal with.
**** Unimodal
Since the Gaussian distribution is concave it can model only one hypothesis. This a curse but also a blessing since the math behind it is simple and unambiguous.

**** high computational complexity
$\mathcal{O}(n^3)$

**** non-convex optimization of the hyper-parameters

*** Sparse Methods
As the computation cost for inverting the covariance matrix is cubic, there are some methods which approximate the solution. One of these methods is  the /Informative Vector Machine/ebib:lawrence_fast_2003 where a subset of samples is selected by maximum entropy. This way this active set can explain the data rest of the data, and using it will still result in a good model.

$$ $$ 

This reduce the complexity to $\mathcal{O}(d^2 n)$ where d is the number of chosen samples.
There is also an /IVM/ method which works for multiple classes.ebib:seeger_sparse_2004

** Gaussian Process - Latent Variable Model

The GP-LVM is an unsupervised learning model to perform a non-linear dimensionality reduction from an observed space $X$ to a latent space $Y$ 
It does this by maximizing the likelihood $$p(Y|X) = p(Y|f)p(f|X)$$ using a Gaussian prior for the mapping $f$. Technically a GP-LVM is a product of Gaussian Processes which model a regression of the mapping from latent space to observed space. \todo{formulas etc.} This means also that if we want to compute the latent position of a new observed sample we have to compute the ...\todo{elaborate GP-LVM}. Using a linear kernel the model generalizes to \todo{PCA} PCA. By using a non linear kernel a non-linear mapping is inferred making it a non-linear latent variable model.ebib:lawrence_probabilistic_2005

#+CAPTION: [GP-LVM example]{GP-LVM example: Human pose reduction (walking activity)}



Analogy LVM <-> marionettes

*** Dual Probabalistic PCA 

Tipping and Bishop, Journal of the Royal Statistical Society (1999)


Assuming $X$ has a Gaussian prior $P(X) = \mathcal{N}(\bm{X}|0,\bm{I})$ 

$$ P(Y|X) = \prod_{j=1}^p \mathcal{N(\bm{y_j}|0, K)} $$

Where $K = XX^T + \sigma^2 I$ is the covariance matrix.
Lawrence noted that this can be interpreted as a product of gaussian processes where the covariance matrix represents the linear kernel ebib:lawrence_probabilistic_2005. By exchanging the linear kernel with a non-linear one, we automatically have a technique for non-linear dimensionality reduction. 

Using the trace properties 
$$ tr(a) = a \text{ , a is a scalar} $$ $$ tr(AB) = tr(BA) $$
we can change the mahalanobis distance term
$ x_i^T K^{-1} x_i = tr(x_i^T K^{-1} x_i) = tr(K^{-1} x_i x_i^T) $
\todo{cite some source for this}

The log likelihood is:

As there is no linear kernel any more this equation cannot be solved in closed form. Therefore we have to do gradient based optimization. But marginalizing over all latent space samples means that we have to include these in the optimization. This fact makes the optimization problem very hard as the dimensionality is high-dimensional -- number of samples $n$ + hyperparametrs -- and has many local minima.

As initially proposed the standard way of initializing the latent space is using /Principal Components Analysis/.

 
*** Back-constraints GP-LVM
One problem with this model is that it does not preserve local distances in the latent space. This is because it tries to explain the data by moving distant samples from the observed space also far apart in the latent space. This problem is addressed by Lawrence et al. in the back-constrained GP-LVM ebib:lawrence_local_2006. A mapping $g_i(y_i) = x_i$ is introduced which constrains the points in latent space to be more near if they are also near in observed space. Instead of optimizing directly on $X$ the back-constrained GP-LVM optimizes the mapping $X=f(Y)$ instead. This back-constrained mapping 

Having this back-constraints also gives us a mapping from observed space to latent space which can be used to project a new sample into the latent space without costly maximum likelihood estimates. 

*** Discriminative GP-LVM
Another improvement in the context of classification in latent space is the Discriminative GP-LVM ebib:urtasun_discriminative_2007. Using a /General Discriminant Analysis/ criterion a prior is being enforced on the latent space which ensures that samples from one class are more clustered and different classes are more separated. This is done by maximizing the between-class separability and minimizing the within-class variability while optimizing the log likelihood of the GP-LVM.
 
*** Other variants
**** Bayesian GP-LVM
An interesting approach for computing the likelihood of the latent variable mapping was proposed in ebib:titsias_bayesian_2010. By using a variational method it becomes possible to marginalize over $X$. Doing so the mapping can be learned together with an \todo{explain ARD} ARD kernel. This way the dimensionality of the manifold can be learned from the data. 

**** Subspace GP-LVM

**** Manifold Relevance Determination
Combining the Subspace GP-LVM with the variational approach and the ARD kernel it is possible to learn the manifold \todo{explain MRD}.ebib:damianou_manifold_2012

*** Advantages
**** interpolation
Because of its probabilistic nature GP-LVM interpolation between two data sample is very natural. ebib:quirion_comparing_2008
**** probabalistic
**** Generative: it can generalize beyond training data
**** non-linear mapping
*** Disadvantages
**** No mapping from observation space to latent space
The idea of the GP-LVM is to learn a mapping from latent space to observation space by marginalization over the latent space. Resulting from this is that we do not have an inverse mapping into the latent space. This fact may be of no importance for character modeling and motion interpolation but in our case it is crucial. An inverse mapping can be computed by using the Back-constrained GP-LVM described above. However one should also keep in mind that using back-constraints inherently changes the latent space as employs an additional constraint on the mapping.

**** Very hard optimization problem
Resulting from the disadvantages of Gaussian Process regarding the optimization of the hyper-parameters the GP-LVM is also very hard to optimize as its objective function is non-convex. But in the case of GP-LVM we have a much larger optimization space due to the fact the we do not optimize only the hyper-parameters, of the mapping Gaussian Process, but also the latent space itself which is of dimenionality $n$. 

This in fact is the biggest problem as it limits its use on real world data, because for more complex manifold structures there will likely be many local minima. For this reason it is crucial to choose a good initialization. Examples are PCA, Local Linear Embedding or ISOMAP.
** Sequence similarity measures

*** Dynamic Time Warping
The /Dynamic Time Warping/ is an algorithm which tries to find a minimal path between two sequences where the path can be warped in the time dimension. The sequences can be of arbitrary length. 

The recursive definition -- excluding some corner cases -- reveals the workings of this method.

#+begin_latex
$$
\text{dtw}_{\bm{x},\bm{y}(i, j) = \text{dist}(x_i, y_j) + \text{min}
\begin{cases}
   \text{dtw}_{x,y}(i-1, j) \\
   \text{dtw}_{x,y}(i, j-1) \\
   \text{dtw}_{x,y}(i-1,j-1) 
\end{cases}
$$
#+end_latex

Where $\text{dist}(x,y)$ is a distance function which tells how close two points are, and $i$ and $j$ are the element indices for the first and second sequence.
The DTW can be computed with dynamic programming and has a runtime complexity of $\mathcal{O}(n m)$ where $n,m$ are the lengths of the two sequences.

It is closely relates to the /Longest Common Subsequence/ where, but instead of maximizing a common subsequence, we minimizing the total warping cost between both sequences.

Since we are not interested in the path itself but in the cost of the minimal path we define the DTW as a mapping from two time series to an real value. We consider DTW to be a distance which is not entirely correct as the triangle inequality does not hold. Nevertheless it gives us a notion of how similar two time series are and since it is non-negative ( $d(x,y) >= 0$ ), symmetric ( $d(x,y) = d(y,x)$ ) and respects the identity property ( $d(x,x) = 0$ ) it can be used to define a meaningful, be it not formally correct, kernel. ebib:shimodaira_dynamic_2001

*** Longest Common Subsequence
The /Longest Common Subsequence/ algorithm finds the biggest non-consecutive subsequence that is contained inside two sequences. Its recursive definition is:

#+begin_latex
$$
\text{lcs}_{x,y}(i, j) = 
\begin{cases}
   \text{lcs}_{x,y}(i-1, j-1) + 1 \text{ if } x_i = y_i\\ 
   \text{max} (\text{lcs}_{x,y}(i-1, j), \text{lcs}_{x,y}(i,j-1)) \text{ otherwise}
\end{cases}
$$
#+end_latex

This can be implemented using dynamic programming and has a run-time complexity of $\mathcal{O}(n  m)$ where $m$ and $n$ are the lengths of the sequences. Several algorithms exist which reduce this complexity by making some kind of assumptions about the data \todo{cite source  survey LCS}.



* Related work                                                     :noexport:
This chapter will introduce some models and their corresponding algorithms for activity recognition. An emphasis is led on methods which work with skeleton data. In the last part a short analysis is done on these methods and some observations are discussed.

** Overview
Activity recognition is a difficult task as we have to make sure our algorithm will discriminate between different classes -- activities -- but also will leave room for inner class variations. These variations are the result of different persons performing activities differently. A simple example is walking, where different person has a different walking style -- also called gait. Also different environments will result in actions to be performed slightly differently. ebib:poppe_survey_2010

There are many methods which learn from videos and try to explain. This approach is very flexible but also has several drawbacks. One of which is that it is very hard to achieve scale and view-invariance. Furthermore inferring the human pose is very difficult and ambiguous. 

For these reasons we will consider only data with pose information in this thesis.
*** machine vision for human activities: a survey ebib:turaga_machine_2008


Generative models such as HMM
Discriminative models such as CRF


Survey on Time-Series Data for classification
** Histogram based approaches
*** Motion history image 
*** Motion energy image
** Dynamic time warping

** A class of space-varying parametric motion fields for human activity recognition

** Action Recognition Based on A Bag of 3D Points
action graph - nodes are shared poses 
** Methods using skeleton features
*** Gaussian Mixture Based HMM for Human DailyActivity Recognition Using 3D Skeleton Features
*** Sung et al. ebib:sung_unstructured_2012
**** Features: Skeleton data + HOG features of RGBD image and depth image 
**** Naive classification: SVM
**** Maximum entropy markov model
Solved via max-flow/min-cut
*** RGB-D Camera-based Daily Living Activity Recognition ebib:zhang_rgb-d_2012
**** Bag of Features
#+begin_src dot :file figures/bag-of-features-approach.png
   digraph pipeline {
     label="pipeline";
     rankdir=LR;

     node [color=blue, shape=box];
  
     feature_extraction;
     k_means;
     vector_quantization;
     centroids[shape=ellipse];

     feature_extraction -> k_means -> vector_quantization -> centroids;

     subgraph {
        label =  "bag_of_features";
     }     
  }
#+end_src

#+RESULTS:
[[file:figures/bag-of-features-approach.png]]

See [fn:2]

**** Features: Structural and Spatial motion
Feature capturing transition between two frames
**** Bag of Features approach (historgram of features)
**** Other: People identification (reidentification)
*** View Invariant Human Action Recognition Using Histograms of 3D Joints
*** Learning Human Activities and Object Affordances from RGB-D Videos 
**** Learning both: activities and object detection/affordance
**** Using Markov Random Field and SVM for learing
*** Eigenjoints ebib:yang_effective_2013
*** Gaussian Process - Latent Conditional Random Field (GP-L CFR)
ebib:jiang_modeling_2014 use GP-LVM to reduce dimensionality of human motion. (earlier approach was Gibbs sampling)
*** Modeling Human Locomotion with Topologically Constrained Latent Variable Models
*** GPDM
In ebib:wang_Gaussian_2005 the dynamics of the latent space is being modeled from time series data. In ebib:wang_Gaussian_2008 this model is being used to model human motion by applying a GP-LVM to the high-dimensional mocap data and simultaneously learning the dynamic transition in the latent space:

                     $$ x_{t_{k+1}} = f(x_{k}) $$

$f(x)$ is being modeled by a Gaussian process.

This model was applied for activity recognition in ebib:jamalifar_3d_2012 where the classification is done through an SVM in the hyperparameter space. (only 2? features)

*** Joint Gait Pose Manifold
The Joint Gait Pose Manifold models the activity and the gait in an common latent space. This way several samples from different persons are modeled with the addition of the gait and do not corrupt the class learning. Each activity is mapped to an toroidal structure where the length represents the activity dynamics and the width represents the gait variation. 

*** Human Action Recognition Using a Temporal Hierarchy of Covariance Descriptors on 3D Joint Locations
** Analysis
Skeleton features are sufficient but other features can be useful:
- hand 
- head pose recognition
- situation awareness
  ...
  
*** Observations
- One observation one can make is that activities are represented by the dynamics of the poses, and thus we try to capture this dynamic model. Several options exist. One way is to use popular graph based probability models, such as Hidden Markov Models, Conditional Random Fields or Actiong Graphs \todo{cite action graph}. Another option is to try to capture the dynamics by appropriate feature extraction. 
  
- Difference between activity and action
  Activities are composed of actions
- Context information can tremendously help in classification of activities (e.g. object detection and human anticipation)
- Skeleton data is sufficient for classification (ebib:ibbt_does_????)
  and also robust to changes in appearance (most state-of-the-art methods work with visual features)
  and also unobtrusive and sensible data doesn't need to be stored (like face features etc.)
- hierarchical learning:
  Some methods learn the actions that a activity is composed of. This practice is also very common in HMM models as they model discrete states and their temporal dependencies
- DTW is a good measure but has several drawbacks, such as in cyclic activities where some motions can be repeated several times
- LLE is not generative therefore LL GP-LVM to preserve smooth map also in latent space

*** Approaches
**** Discriminative Sequence BCGPLVM
Use this to find the activity
***** DTW between walking and walking backwards very big ...
***** not taking temporal dimension into account
**** GPDM
***** approach to classify by hyperparameters not optimal
**** Classify by dynamics of the skeleton (this should bring good classification)
***** GPDM can model the dynamics of the movement
***** has good properties (Gaussian processes)
***** has intrinsic dim reduction
***** ?? shared GP-LVM to model different activities in the same latent manifold ??
*** Problems and solutions
**** limited sample data - probabilistic model + discriminative
Probabilistic (and generative ??) models are more accurate using fewer samples, because they model the probability directly ...  
**** high dimensional - dim reduction(gp-lvm)

**** classification - BC GP-LVM + discriminative
**** time series data - GPDM
An can be modeled as a sequence of consecutive poses. Hence a dynamical model. By using a dynamical model classification becomes more discriminative. 
**** confidence is important !!!
Using a probabilistic model (especially Gaussian processes) we also get a confidence which in turn can be used for active learning
**** high dim. noise => GP-LVM is very robust because of the nature of optimization (distance is preserved instead of locality)
*** Assumtpions
**** Skeleton tracking is correct and stable
For the algorithm we assume that the skeleton extraction from RGBD data works as expected.
This is far from the truth with current skeleton tracking algorithms but we also get confidences of the poses.
This way we can prune a large number of incorrect poses and because we model the dynamics and do not compare poses this is not a big problem.
**** Smooth skeleton transition !!!
**** Correctly labeled samples (no outliers)
*** Ideas
**** Use hand and/or head features
***** Head direction is important
***** Hand structure is very important for most tasks
***** Object interrelation ???
***** Use HOG for hand features only

**** bag of features 
- no time dependency
- no online capable because of k-means clustering

*** GP-LVM for human motion
As the space of human motion is high-dimensional (spatio-temporal) dimensionality reduction is crucial for a number of models dealing with human motion (e.g. ebib:fan_Gaussian_2011l).
The GP-LVM preserve the distances in the mapping and are therefore suitable to model human motion with high noise of the poses see Urtasun DGPLVM
Newest addition is ebib:jiang_modeling_2014



* Approach: KMeans clustering approach
As a starting point, we choose to re-implement a working method with a good performance on this data set. Therefore we choose an existing algorithm based on the /bag-of-features/ approach published in 2012 ebib:zhang_rgb-d_2012.

The idea is illustrated in Figure [[fig:bof-approach]]:
- Define features which capture the structure in a time instant along with the local displacement of the skeleton. There are two types of features that are extracted. First the structural configuration is captured by the difference vector between each joint pair. Second the local motion is captured by the difference vector for frame $t$ and $t-1$ for each joint. This way the feature represents the current configuration and the current motion performed for every frame. The feature vector is of size 360.
- From all poses find the most $k$ prevalent ones. This means clustering the feature space and finding the mean vectors for each cluster. This is done by the K-Means method.
- Quantize each activity by these poses. For each activity, each frame is being mapped to a cluster mean by nearest neighbor. Doing so we have a sequence of the mean poses for each activity.
- Compute a fixed sized vector that represents the distribution of each mean pose. By computing the histogram over the previous sequence and normalizing we capture the occurrence of each pose representing the feature clusters  (bag-of-features). 
- Perform classification using this new feature vector. Using a linear /Support Vector Machine/ we learn the activities along with their corresponding labels.



The above algorithm works very well in practice. This can be explained by the fact that the mean poses are very distinct for different activities. This means that they capture the most discriminative poses of the activity which can be robustly recognized.

 Also we tested this method with only partial data and it performs relatively well. We used 100 frames uniformly sampled from each activity.




** Cornell Daily Living Activities dataset

We will use the "Cornell Activity Datasets (CAD-60 & CAD-120)"[fn:1] to learn and evaluate 
the performance of our implementation. 
This dataset is challenging as it contains complex daily living activities, some of which are very close together. There are four persons each performing 13 activities. 
The activities /brushing teeth/, /brushing mouth

One person in the data set is left handed and therefore the recognition ability drops considerably in this case. One way to make the method more robust for this case is to also learn the mirrored data. We do not use this approach as we wanted to compare our extensions with the original paper.

The data set consist of an sequence of frames which include: 
- Image data
- RGBD data
- Skeleton information: (joint position and orientation)
- annotated meta information (e.g. activity)


** Robot Operating System (ROS)
The /Robot Operating System/ \todo{cite ROS} is a middleware which is intended to consolidate and define a layer for the implementation of complex robotic systems. It has a variety of drivers for different sensors and actors and defines a /node/ based interface for communication between different sub-modules. 

Each node can define a communication interface by defining message types, topics and services. This way a complex system is split in several small nodes and, because of this modularization, it is easier to add, exchange, work on and test different parts and functionality.
The nodes can communicate using either pre-defined /topics/ which have a message type or /services/ which can also have some own defined type. A node can subscribe to a topic and each message that is then published on this topic will result in an callback.

Transforms between coordinate frames ...

A /bag/ file captures all messages which are being send along with the whole topic net. This way real world data can be recorded and be played back. This is very convenient for debugging or system integration.

** Implementation
For the implementation we used Python with the scipy and scikit-learn libraries. For K-Means we used the mini-batch implementation of K-Means which is expected to perform worse than the passive variant, but also is much faster. As described in the paper we also used a linear SVM with an /RBF/ kernel for classification.

** Integration into ROS
For real time extraction of the skeleton we used the openni_tracker module. /todo{cite} This module read the values of the OpenNI nite /todo{cite} skeleton tracker driver and transforms the coordinates to a ros specific depth camera frame. Then it publishes these transforms as a TF message. Because of the fact that each joint TF broadcast is not synchronized we modified the module to publish the pose as an atomic message containing the skeleton positions for each frame. We also did not use any transformation, as we wanted to use the Cornell data set which is recorded with the raw data coming from the RGBD sensor. This way we could test the performance of the algorithm for online recognition without tedious creation of a new data set.

We publish the pose on the topic =/openni_tracker/pose= having the message type of an array of float32.

We implemented a new ros module called /activity_recognition/ which subscribes to the above topic saves a number of poses and every three seconds performs a classification on the sequence. As the provided dataset is relatively large the learning time is several minutes. The most time takes to parse the data files and extract the features. As we did not want to do this every time we serialized a learned model and loaded it every time the module starts. This way it is also possible to learn different saved activities and begin the recognition without waiting for the model to be re-learned. 

** Shortcomings
The skeleton tracking is very noisy. We observed very big variations between subsequent frames. Therefore we performed a discrete Gaussian filter smoothing for each sequence. Unfortunately the recognition rate did not improve.

We observed that the number of prevalent poses is not sufficient to capture the variances inside some classes. 
For this reason we performed K-Means for each class of activities separately and used the ball-tree nearest neighbor algorithm quantize the sequences for the recognition. With this it is more likely that same activities will fall to the same representative poses as they are more evenly distributed between the classes. Moreover this allows us to extract more mean poses as the K-Means algorithm has to run only on the samples of each class separately.

The /bag-of-features/ approach performs very well but it does not capture the order of the underlying poses. Instead by performing histogram pooling, it has a notion of how prevalent each pose is for every activity. 

To circumvent this we modified the method to classify with the /Longest Common Subsequence (LCS)/ algorithm. Instead of performing a histogram pooling we classify each quantized sequence using the average /LCS/ distance for each class. The standard algorithm for the /LCS/ for two sequences is implemented, just like in the case of the /DTW/, with dynamic programming.  In our case this is not needed as we already know that different activities will contain different poses. For this reason we can simply remove all elements which are not in the intersection of both sequences as a pre-processing step.

A second idea was to compare the sequences using /Dynamic Time Warping/. For this we chose as a measure between each mean pose the euclidean distance in feature space, which will give a good approximation in the case that the clusters are located far away. As the /DTW/ has a complexity of $\mathcal{O}(n*m)$ we took every fifth element from the sequence for the calculation.
Also by pre-computing the distance matrix the distance operation is a simple look-up operation and the algorithms is fast enough. 


One serious drawback of this approach is that only a fixed time interval can be classified. There is no way to robustly recognize transitions between different activities. For this reason we tried another approach which uses /GP-LVM/ to reduce the feature space and can find the centroid for an activity in this space.

** Evaluation
We performed 4-fold cross validation using each person as test data and the other three persons for training.
We achieved a comparable precision rate of 84% and recall rate of 84% as stated in the paper. Using the /LCS/ measure we achieved an precision and accuracy of 88%.

We also tested the prediction rate using only 100 frames for the prediction. By uniformly sampling 50 intervals from each test sequence the algorithm achieved the same average accuracy and precision of 88%. It can thus be argued that the most discriminative information for the classification task is inside the powerful features that are extracted and the representative poses produced by the clustering..  

* Approach: Discriminative Sequence Back-Constrained GP-LVM
As discussed earlier the simple /bag-of-features/ approach has its limitations as it is not capable of identifying activity transitions. To deal with this problem we choose to implement another algorithm, capable of classifying a sequence in real time and inherently taking the alignment of the sequences into account. 

** Discriminative Sequence Back-Constrained GP-LVM

In the paper "Discriminative Sequence Back-Constrained GP-LVM for MOCAP Based
Action Recognition"ebib:_discriminative_2013 the authors propose a method for classifying MOCAP \todo{cite mocap} actions.

The MOCAP database consists of a large number of different activities performed by human actors and recorded using motion capture devices. The information recorded is comparable to the skeleton representation but contains more data and is virtually noise free.

The method proposed in ebib:_discriminative_2013 is illustrated in Figure [[fig:discr-seq-approach]]

The idea is to perform a dimensionality reduction on the skeleton data, resulting in a much compact representation. By introducing a /DTW/ based sequence alignment kernel similarity measures can be defined for the activities. By using this similarity measure for the sequences in the observed space and constraining the optimization to preserve this measure the local distances between the sequences are transferred into the latent space. The latent points of similar sequences are thus located nearby and the centroids of similar activities are distributed more close to each other. Then instead of using back-constraints to map a single pose sample into the latent space, we can compute the centroid in the latent space directly from a sequence of poses.




#+begin_src dot :file figures/seq-gplvm-approach.png
   digraph pipeline {
     label="Pipeline: Sequence back-constrained GP-LVM pipeline ... CITATION";

     node [color=blue, shape=box];

     subgraph clusterLearning {
        style = filled;
        label =  "learning";
        feature_extraction -> gplvm -> latent_space -> centroids;
        sequence_constraints -> gplvm;
        discriminative_constraints -> gplvm;

        discriminative_constraints [shape=ellipse, label="discriminative  constraints"];
        sequence_constraints [shape=ellipse, label="sequence constraints"];
        { rank=same; gplvm; sequence_constraints; discriminative_constraints; }
     }

     centroids -> SVM;

     subgraph clusterRecognition {
              label = "recognition";
              sequence_mapping -> SVM -> activity_class;           
     }
  }
#+end_src


#+Caption: Illustration of the "Discriminative sequence back-constrained GP-LVM" approach. Learning is done by training a GP-LVM together with the sequence-back constraints and discriminative constraints. After that an Linear SVM is trained by the class centroids and the corresponding labels. Recognition is performed by computing the centroid of a new sequence through the learned back-constraints and predicting with the SVM.
#+Label: fig:discr-seq-approach
[[file:figures/discr-seq-approach.eps]]


The sequence back-constraints have two advantages:

First all the sequences have a meaningful clustering in the latent space and thus the mean (centroid) of each sequence is a good representation.

Second by also learning the back-constraint it is possible
to calculate the centroid of a sequence in the latent space directly without maximizing a likelihood. This in turn is being used to infer the centroid for an activity in the real-time classification for actions.

The authors validated this approach on the MOCAP dataset using 7 different actions (Run, Walk, Jump, Throw-Toss, Sit-Stand, Box, Dance) and achieved an average recognition rate of 72.9%.

*** Sequence back-constraints
The mapping is defined as a linear combination of the /DTW/ distance between every other sequence. For every latent dimension $q$ we have:

              $$ g_{q}(Y_s) = \sum_{m=1}^{S} a_{mq} k(Y_s,Y_m) $$

where the similarity measure is $k(Y_s, Y_m) = \gamma e^{\text{DTW}(Y_s, Y_m)}$. This measure can be interpreted as a sequence alignment kernel. The
measure is to be preserved in the latent spaces.

       $$ g_q(Y_s) = \mu_{sq} = \frac{1}{L_s} \sum_{n \in J_s} x_{nq} $$

Therefore we need to perform a constrained optimization for the /GP-LVM/.


*** Discriminative GP-LVM
Furthermore, by applying the Discriminative GP-LVM we ensure that poses of different activities are separated from each other and poses from similar activities are located closer together. This ensures that the centroid of an activity is more informative and thus discriminative. The Discriminative GP-LVM works by minimizing the between class similarity and maximizing the innner-class variance ebib:urtasun_discriminative_2007.

The two criteria for optimization are:

- The distance between the classes

$$ S_b = \sum_{i = 1}^l \frac{n_i}{n} (\bm{\mu_i} - \bm{\mu}) (\bm{\mu_i} - \bm{\mu})^T $$


where $n$ is the number of samples, $n_i$ is the number of samples for class $i$ and $l$ is the number of classes. Furthermore $\bm{\mu_i}$ is the mean of the class and $\bm{\mu}$ is the mean across all classes. 

- The variance within each class

$$ S_w = \frac{1}{n} \sum_{i = 1}^l \sum_{j = 1}^{n_i} \frac{n_i}{n} (\bm{x_{i,j}} - \bm{\mu_i}) (\bm{x_{i,j}} - \bm{\mu_i})^T   $$

where $\bm{x_{i,j}}$ is the $j$-th sample from class $i$.

The variance within each class $|S_w|$ should be minimized and the distances between the classes $|S_b|$ should be maximized. This is done by combining both condition into a sole criterion and maximizing it:

$$ J(X) = tr(S_w^{-1} S_b) $$

This criterion is added to the likelihood of the GP-LVM with a parameter $\lambda$ which decides how much weight the discrimination should take in the optimization. If we make the model more discriminative we could break the learning of the non-linear manifold. On the other hand if the value is small the contribution will bee to minor and we will not gain any discrimination between the classes.
Recognition is being done by applying the mapping above to the
new sequence and using a SVM in the latent space.

*** Advantages
This methods maps each pose from every activity inside the same latent space, which ensures that the mapping captures a non-linear manifold which is categorized by all activities. 
Recognition can be done in real time by using the learned back constrained. The centroid in the latent space is being calculated for the whole sequence and classified by the SVM. 
Also incomplete trajectories can be classified. When there is an activity transition the centroid will cross the decision boundary of the /SVM/ and be naturally classified to the new corresponding activity. 

*** Shortcomings
As all activities are modeled inside one latent space it is very difficult to find a non-linear mapping from latent to observed space. The standard approach for optimization in the /GP-LVM/ is using the /Scaled Conjugate Gradient/ method. As the optimization for /GP-LVM/ is determined by the above similarity measure and the discriminative criterion finding a good minimum is very difficult. It is thus highly likely that performing a gradient optimization will be stuck in an local minimum. The authors in ebib:_discriminative_2013 argue that initializing with a more sophisticated dimensionality reduction technique is a necessity. In their work they use the /ISOMAP/ and the /Locally Linear Embedding/ methods. 

Also one problem with the real-time recognition is that determining when exactly an activity has ended/begun is very difficult. Also as we do not know how long a sequence is we have to calculate the centroid for several time frames using a sliding window approach.

*** Extensions:
**** Learn pose together with local motion to capture dynamics
The GP-LVM learns a mapping for each pose but does not consider velocities and accelerations. If we take a pose along with its first and second moments as the high-dimensional space we allow for the temporal displacements to be also modeled. The latent space will represents the pose along with the local motions and the DTW kernel in the constraint will also captures the dynamics of the activity. Due to the difficult optimization and the high complexity of the data set we could not find a good local minimum with this approach.
**** Use mahalanobis for the DTW
As described in section [[Dynamic Time Warping with Mahalanobis Distance]] we wanted to use a modified version of the /DTW/ for learning the sequence back-constraints. But due to our tests the mahalanobis inspired /DTW/ did not perform any better for our chosen features.

** Feature extraction
Regardless of the chosen algorithm the features used for learning will have a big impact on the performance of the model. Therefore it is imperative to extract discriminative features from the skeleton data.

We get the joint positions and the angles between them in the camera frame defined by the used depth camera (.e.g Microsoft's Kinect).When extracting features we have to make sure that we have view invariant features of the skeleton. We want these data in the frame of the skeleton.

One way to achieve scale invariance is to normalize all link lengths in respect to the torso link. This correct for variances of skeleton lengths in different persons. To make the pose view invariant we have to define a local skeleton frame which captures the skeletons /orientation/ in the world coordinate system.

#+Caption: Sketch of the local skeleton frame inside the camera frame. The rotation matrix $\bm{R}$ and the translation vector $\bm{t}$ define the needed transformation to change from camera coordinates to the local skeleton coordinates
#+Label: fig:skeleton-frame
#+ATTR_LATEX: :width 10cm
[[file:figures/skeleton-frame.eps]]

Another way to achieve view invariance is to not consider the 3D points of the joints all together but instead to take only relative features. These can be, for example the angles or distances between two adjacent joints. An interesting approach is used in ebib:theodorakopoulos_pose-based_2014, which is to define a polar coordinate frame for each joint and use only two angles, which define the orientation of the joint in a polar coordinate frame, as features. This way we also reduce the observation space. 

As discussed in [[Related Work]] many methods also make the extracted temporal features (e.g. Eigenjoints). However since we want to include the dynamics in our model we do not extract such features explicitly.

We selected a 3D point cloud of the joints in the skeletons own coordinate frame as features. The reason for this is that we believe the 3D point cloud to be more linear than relative features, which in turn will help when optimizing the model. Figure [[fig:skeleton-frame]] shows this approach. We chose the two vectors -- torso to right hip and torso to left hip -- to define our local coordinate system. By normalizing and computing the cross product we have also the third vector which points to the walking direction of the skeleton. 


** Implementation
As there was no publicly available source code and we wanted to integrate the code with ROS we choose to implement this method in Python. We used the /GPy/ library from the ... Sheffield University \todo{cite GPy}. 

To implement the Discriminative GP-LVM constraints by porting the code from Prof. Urtasun's matlab code to Python and integrating it with /GPy/. 

Doing so we encountered several problems with the current numpy and scipy libraries dealing with sparse matrices. As of now there is no way to perform a fast multiplication of sparse matrices and of block-diagonal matrices. The only solution to this is by manually implementing an algorithm. But doing so one looses all the advantages of the BLAS and LAPACK integration of numpy.

To enforce the sequence back-constraints we implemented a constrained optimization by adding Lagrangians to the objective function. This way the weight parameters for each sequence alignment kernel were learned. 
** Dynamic time warping with mahalanobis distance 
The Dynamic Time Warping algorithm is a prominent and very effective choice for computing similarity between two sequences. Using this measure as a sequence alignment kernel the methods aligns similar sequences closer to each other. The effectiveness of the recognition is determined by the accuracy of this alignment kernel.
 
The issue with this approach, in the context of activity recognition, is how to define the distance metric between two poses. This metric is crucial for the /DTW/ to find an optimal path.
Popular choices for the distance function is the euclidean distance, if 3D points are used as features and which we used in our implementation, and the angular distance for angles. The problem with these two distances is that they are just the sum of the individual feature differences. As the dimensionality grows this metric becomes less informative. 

In the case of human poses we have a certain notion of which poses are similar and which are far apart. Maybe this is due to the fact that we inherently know -- or classify -- to which activity the pose corresponds to and have therefore some notion of closeness with respect to an activity which cannot be approximated with the euclidean distance. Poses from different activities will most likely also seem to be more or less similar depending on how similar the actions are.

One idea to transfer this knowledge is by using the Mahalanobis distance instead of the euclidean distance when computing the similarity of two pose sequences. By computing the covariance for each activity we have some notion of the variance across all feature dimensions for a specific class. This way we can capture -- to some extent -- the variability for each class. Now we can compute a similarity measure with a new sequence $x_new$ for each class and each sample of this class. Thus we can define a notion of measure between a class and a new sample by:

$$ s(j, \bm{x_{new}}) = \frac{1}{|C_j|} \sum_{\bm{x} \in C_j} \frac{\text{DTW}_{\text{mahalanobis}(\bm{\Sigma_j^{-1}})}(\bm{x}, \bm{x_{new}})}
{min(|\bm{x_i}|, |\bm{x_{new}|)}} $$

where $C_j$ is the set containing all class sequences and $|C_j|$ is the number of sequences in class $j$. The normalization factor $min(|\bm{x_i}|, |\bm{x_new}|)$ makes sure that the minimum cost computed by the $\text{DTW}$ is proportional to the smallest sequence.

This way the distance error is distributed by a way defined by the variance across each dimension.

A similar idea was also proposed in the context of handwritten signature verification in ebib:qiao_learning_2011, which uses just one covariance matrix.
The covariance matrix is determined such that, just like in the case of Discriminant GP-LVM, it maximizes the variability between classes and minimizes the difference for samples in the same class. 
In contrast to our approach the overall covariance matrix may define a more meaningful and discriminative measure but it is also more difficult to update when performing online learning and when learning a new class (novelty detection).

*** Implementation
We wrote a simple version of the Dynamic Time Warping in Python using dynamic programming and following the recursive definition in chapter [[Dynamic Time Warping]]. As the variance for some feature dimensions can be zero the constructed covariance matrix does not have full rank and thus cannot be inverted. We mitigate this problem with an approximation of the inverse by computing the pseudoinverse.



** Evaluation
Our tests on the Cornell Daily Living Activity data were unsuccessful as the optimization failed to find a discriminative latent space.
We believe that the many constraints on the optimization and the highly variant data is very hard to optimize. 

Another reason for this could be that the activities in the Cornell data set are more complex. The MOCAP data represents action which could be described more easily with a non-linear manifold. In contrast most daily living activities consist of several and, also in their inherent structure, different actions. The /DTW/ measure is therefore not suitable to capture the similarity between two complex activities.

It can be argued that the performance of the /GP-LVM/ is strongly dependent on the initialization. This was stated also in the original ebib:paperlawrence_gaussian_2003. It seems that the objective function is highly-nonlinear and it is very likely that the optimization will find a local minimum in the vicinity of the starting position.

For these reasons we choose to implement a new model based on motion flow fields which will be learn for each activity separately.

* Approach: GP-Latent Motion Flow 
It can be argued that the mean poses computed in the /bag-of-features/ method capture the most probable pose and motion tendencies of an activity. The good performance of the algorithm can be attributed to this fact. This can be explained by the local motion descriptor and the structural descriptor which are good representations for the current pose.  We want to find an algorithm which performs dimensionality reduction, like in the case of the /Discriminative sequence back-constrained GP-LVM/ but also captures the motion tendencies for each activity class.

Many models use GP-LVM to reduce the high dimensional space into fewer dimension. These approaches make the problem more feasible but the issue remains how to do classification for time-series data. Human motions are mostly characterized by the dynamics of the model (temporal dimension). So we have to compare trajectories in the latent space. One idea is to learn a /Gaussian Process Dynamical Model/ for each activity. This way we will have a function of the trajectory. This method is very powerful but using it will very likely not give good results in our case, due to the fact that more complex activities do not necessarily resemble the same trajectory. If we take the activity "cooking" as an example, there is no main trajectory that is being followed. Moreover this activity is defined by its local motion tendencies and sub-actions. Also a trajectory based approach will not be able to model cyclic action inside an activity which can be repeated an undetermined number of times.

One idea to solve these problems is to learn a motion flow field inside the latent space. This can be with a similar method called /Gaussian Process Regression Flow (GPRF)/ proposed in ebib:kim_gaussian_2011. The classification can be done using first and second order dynamics which should give better results. Going further the activity itself is characterized by the first and second moments of the trajectory function. By explicitly modeling the velocity of the trajectory we can take changes in the joint movement into account.

** Gaussian Process Regression Flow
 ebib:kim_Gaussian_2011 can be used to model the trajectories in the latent space.



\todo{explain GPRF}

** GP-Latent Motion Flow

The GP-LMF method is inspired by this model. The difference being that in the case of activity recognition we do not know the starting position and also the trajectories can have significantly different lengths. For this reason it is very difficult to normalize with respect to the time dimension.
Nevertheless, resulting from the properties of Gaussian Process regression, we have also a dense mean flow field and dense variances. This allows us perform efficient and robust online recognition in the latent space.

This model is attractive for two reasons. First real-time classification of incomplete trajectories is possible. Incomplete not only in the sense of the first part of an activity but any interval of an activity, which could be also somewhere in the middle of the sequence. Second it is possible to do online learning by simply adding the new class as a new flow field to the pool of GP regressions. It is very difficult to adjust other models, which rely on the mapping between latent space to observation space, for online learning, because of the problem that we can get stuck in a local minimum when optimizing the parameters of the GP-LVM.

The idea is to learn a motion field in the latent space for each activity. This can be achieved by learning the velocity function of the latent point just like in the GPRF model presented above. With the difference that we do not use the spatio-temporal domain but only the spatial domain for the latent space. The reason being that we do not have starting and ending positions for each activity and also the lengths can be variable. On top of that we also want to recognize an activity which is being interrupted by another activity, so we cannot fix the lengths of the trajectories. 


Each activity has its own flow field. Recognition and prediction is done by calculating the energy of the currently moving point with each different field. The field with the minimum energy represents the most probable activity as the point follows more closely its "current" of motion.

Variances in the speed of performing an activity can be modeled by giving the point in the latent space a mass which can be adjusted in real time.
When a point has greater mass then it needs more energy to be propagated through the flow field (the overall activity is slower) and vice versa.

As we use the /GP-LVM/ and a /GP regression/ way we have two indicators for recognizing unobserved data. The first one is the variance of the back-constraint mapping. If it is high we know that current sample is far apart from the observed ones. The second is the variance of the /Gaussian Process Regression/. If this value is high we know that we didn't see any sample in the latent space with the current motion. Therefore, with this two indicators, we have a notion of how new a sample and its current motion are.

Another advantage of this method is that activities with repetitive motions, such as walking or running, can be learned without using periodic kernels or without resorting to model them explicitly. Repetitive motions can be seen as just multiple samples of the same motion which define the flow field.

#+begin_src dot :file figures/gplmf-approach.png
digraph pipeline {
        label="Pipeline: Gaussian Process - Latent Motion Flow";

        node [color=blue, shape=box];

        subgraph clusterLearning {
                label = "learning"
        
                subgraph clusterDimReduction {
                        style = filled;
                        label =  "dim. reduction";
                        feature_extraction -> gplvm -> latent_space;
                        back_constraints -> gplvm;

                        back_constraints [shape=ellipse, label="back constraints"];
                        { rank=same; gplvm; back_constraints; }
                }

                latent_space -> numerical_derivative -> GPs -> flow_model;
                
                
        }

        energy_computation -> flow_model [arrowhead=dot, style=dashed];

        subgraph clusterRecognition {
                label = "recognition";
                online_sequence -> energy_computation -> class;           
        }
}
#+end_src

#+Caption: Illustration of the "Gaussian Process - Latent Motion Flow Field" approach.
#+Label: fig:gplmf-approach
#+RESULTS:
[[file:figures/gplmf-approach.png]]

Figure [[fig:gplmf-approach]] illustrates this approach.
- For every activity class the latent space representation is learned. 
- Then we compute the numerical derivative of the sequence inside latent space. 
- 


** Requirement: Smooth latent space
It is imperative for the /Gaussian Process/ regression that the latent space is smooth. It this is not the case than different poses which are located nearby can have completely different gradients.  
As the /GP-LVM/ preserves distances rather than locality, it is very likely that there will be no smooth mapping in the latent space. One approach to constrain the latent space to be smooth is using the back-constraints. 

** Learning the flow field

The initial idea was to learn a general dimensionality reduction for a high number of varying activities and work with only one latent space. But as we saw in Chapter [[Approach: Discriminative Sequence Back-Constrained GP-LVM]] such an approach is very difficult to realize, because of the difficult optimization task. Another problem is that the it is very difficult to learn a smooth mapping in the latent space. This is described more deeply in ebib:urtasun_modeling_2007-1 where the authors try to incorporate the optimization criterion of /Locally Linear Embedding/ together with the a back-constrained /Gaussian Process Dynamical Model/. As this approach needs also prior knowledge and is very complex we decided to learn each activity separately. Future work should deal with the possibilities of learning a unified latent space at it will allow us to learn different flow fields in the same space and we will not have to perform a heuristic normalization of the different flow fields.
 
One problem we encounter by learning the motion flow field from several samples is complexity of the /Gaussian Process/. There are two solutions for this. The first one is to use a sparse GP model. The second one is to sample points from all samples and use only those that are most suitable for the regression. If we take /IVM/ as the sparse GP model both approaches can be seen as equivalent as the /IVM/ will automatically take the most informative samples.

*** Effects of the hyperparameters

Changing the /lengthscale/ defines how much each point is contributing to the regression process. It can be interpreted as a smoothness factor which governs how strong the interpolation of the flow field is performed on the latent points.

Changing the signal variance controls how much 





** Interpretation
The proposed model has a natural interpretation. A point represents a pose in latent space and an activity is a trajectory in time inside the same space. With the flow field we learn the motion tendencies for each pose. When performing recognition we let the current point traverse each separate flow and compute the needed energy. If we consider that the point has a mass we can model the speed at which activities are being performed. This way we can recognize when a point leaves an activity, which represents a /motion current/, and passes over to some other activity.

The model captures the changes in velocity which is comparable to the motion history images discussed in Chapter [[Related Work]].

** Recognition


The first approach for computing the energy for a new sequence and each flow field was to compute the dot product of the actual gradient with the vector from the flow field at the latent point mapped from the back-constrained mapping. This quantity is then added to the weight of the particle.  

Inspired by the particle filter method our recognition approach was to have a particle for the latent space of each activity. In every time step the particle is being updated with the above described probability. Then all particles are normalized. This way we ensure that the particle represents the probability that the current action is being performed. If it respects the flow field it will accumulate more weight and due to the normalization the other particles will become smaller. 

** Advantages
*** Recognition
The current activity is being mapped into the latent space. Through the learned back-constrained. The recognition is being performed solely in the latent space. By propagating the current position by each flow field we can calculate the next possible pose. By comparing the similarity considering the variances we have a measure of how well the current activity resembles each flow field e.g. learned activity. By doing this for each separate activity class we can maintain a probability how likely it is that the current motion tendencies resemble each learned class. 

*** Prediction
If we have detected the activity predicting is simply a matter of propagating the pose through the flow field by taking the mean of the GP. This way short-term predictions are possible. Also the prediction is updated every time the point changes its position in the latent space. 

*** Multiple Hypothesis Prediction
Since we have a GP representing our flow field we can predict future point positions with the mean value. Moreover also having informative variances we can sample several possible trajectories. This can be accomplished using an particle filter. Hence we can have multi-hypothesis predictions along with their probabilities.

*** Online learning

*** Active learning


*** Natural interpretation
*** Novelty detection (anomaly detection)
In ebib:kim_Gaussian_2011 the authors present the ability of the GPRF model for anomaly detection. 
This approach is also suitable for finding new classes as the above energy value can be used to recognize novel activities. The reasoning is that if we cannot find an flow field with a small energy the activity has to be unobserved.
*** In comparison to the GPDM it can model cyclic activities

** Implementation
We used non-sparse /GP-LVM/ with a back-constrained mapping to learn each class of activities. The back-constrained has two purposes. The first is that we need a backward mapping to map new poses to the latent space. The second is that the latent space must be smooth. 

** Problems
*** Dimensionality reduction
Performing a non-linear dimensionality reduction is no easy task. Testing was done with only two dimensions as it easier to visualize the latent space and the resulting flow fields.
A latent space with higher dimension will naturally make the reduction more robust and the field will have a more natural interpretation....


*** Stable class mean flow field
When learning a stable flow field from several samples the field can degenerate with the inclusion of strong variable paths. Therefore it is important to ensure that the algorithm learns stable paths. This can be achieved by sampling uniform random sampling from all samples of the same activity.

\todo{active learning - problem ??}

** Evaluation

At first we concentrated our efforts for learning with the MOCAP data. In theory the data collected from the kinect should be equivalent. One difference is the high noise in the pose estimation, but due to the fact that the GP-LVM preserves distances rather than locality this problem is mitigated to a certain degree. For most activities, such as walking, running and jumping it was possible to learn a representative flow field. For more complex actions the optimization could not find a smooth mapping. This could also be due to the used /RBF/ kernel for the backward mapping.

Unfortunately we were not able to perform an appropriate dimensionality reduction.

This several reasons:
- Due to the fact that we want to also learn a backward mapping (observed space to latent space) we have to initialize the latent space with this mapping. For this reason we could not define a more appropriate initialization such as /PCA/ or /LLE/.

- The Cornell dataset contains many activities with many samples. Using the /IVM/ variant helps in some respect to circumvent this problem but also introduces an approximation. 

ebib:urtasun_modeling_2007-1
ebib:bitzer_kick-starting_2011
ebib:urtasun_3d_2006

The author in Exploring model selection techniques for nonlinear dimensionality reduction also suggest to use ISOMAP or LLE to initialize the GPLVM and argues that direct optimization of the GP-LVM is very difficult.


Probabilistic Feature Extraction fromMultivariate Time Series using Spatio-TemporalConstraints

* Conclusions and Outlook
** Summary
In this thesis we covered the issue of performing online activity recognition from skeletal features. We began by re-implementing an existing method, which works by extracting representative poses and performs a histogram pooling over the quantized pose sequence.
** Lessons learned
*** Dimensionality reduction for all activities is very difficult (also with extra constraints)
*** Dynamics is a good measure for classification of human activities
** Contributions
*** Implementation of 
*** Advantages and Disadvantages of dimensionality reduction with GP-LVM for human motion in the context of activity recognition
*** Implementation of the Discriminative GP-LVM with python 
We ported the matlab code provided by Prof. Urtasun into python and integrated it with the GPy library
*** Implementation of the Sequence Back-constraints 
We used Lagrangians to implement a constrained optimization of the likelihood function
*** Improvement of the DTW measure with the mahalanobis distance
*** A novel approach for activity recognition
With the /GP-LMF/ we presented a novel method for online recognition of complex activities. 
*** Introduction of an energy minimization approach for online recognition of complex activities
** Outlook
*** Implementation of the GP-LMF using spatio-temporal GP-LVM
As described earlier the GP-LMF approach failed, due to the fact that the optimization of the GP-LVM with back-constrained did not result in a smooth latent space. One possibility to solve this problem is by implementing the /Spatio-Temporal GP-LVM/ as described in \todo{cite spatio temporal GP-LVM}.  


*** Semi-supervised activity learning by automatic segmentation of activities
If the problem of a smooth latent space can be solved the /GP-LMF/ method can be used to perform an automatic segmentation of observed and unobserved activities. As discussed earlier, since we have two good indicators of the uncertainty of both, the poses we see and the motion tendencies, we can segment a time interval into "known" and "unknown" activities. In conjunction with online learning, this will greatly reduce the time this method needs to learn appropriate flow fields for a number of classes. 




* Latex end                                                        :noheading:
#+begin_latex
\listoffigures
\bibliographystyle{plain}
\bibliography{bibliography}
#+end_latex


* LAB                                                              :noexport:
** Classification
*** Dataset management
#+begin_src python
import glob
import os
import numpy as np


data_set_indices = []
# indices of positions of first 11 joints (joints with orientation)
# 9 ori + 1 conf   +   3 pos + 1 conf = 14 
for joint in range(0,11):
  for x in range(10,13):
    data_set_indices.append(1 + joint*14 + x);

# indices of hands and feet (no orientation)
for joint in range(0,4):
  for x in range(0,3):
    data_set_indices.append(155 + joint*4 + x);
        

default_data_dir=os.getenv("HOME")+'/data/human_activities'

      
class DatasetPerson:

  data_dir = "";
  person = -1;
  direcotory = "";
  activity_label = dict();
  classes = list();
  activity = ''
  data = None

  def __init__(self, data_dir=default_data_dir, person=1):
    self.data_dir = data_dir;
    self.person = person;
    self.directory = data_dir + '/data'+ str(person) + '/';

    # read labels
    with open(self.directory + '/activityLabel.txt') as f:
      self.activity_label = dict([filter(None, x.rstrip().split(',')) for x in f if x != 'END\n']);

    self.classes = list(set(self.activity_label.values()));
    self.activity = self.activity_label.keys()[0]
    self.load_activity(self.activity)


  def load_activity(self, activity):
    self.activity = activity
    file_name = self.directory + activity + '.txt';
    self.data = np.genfromtxt(file_name, delimiter=',', skip_footer=1);

  def get_processed_data(self):
    data = self.data[:, data_set_indices];

    # take relative position of the joints (rel. to torso)
    for row in data:
      torso_position = row[6:9]
      for joint in range(0, 15):
        row[joint*3:joint*3+3] -= torso_position

    return data

  def get_pose(self, frame):
    return Pose(self.data[frame])
#+end_src

*** Visualization
**** Skeleton structure
#+begin_src python
LINKS = {'torso' : ['neck', 'left_shoulder', 'right_shoulder', 'left_hip', 'right_hip'],
         'neck' : ['head'], 
         'left_shoulder' : ['left_elbow'],
         'right_shoulder' : ['right_elbow', 'left_shoulder'],
           'right_elbow' : ['right_hand'], 
           'left_elbow' : ['left_hand'], 
           'left_hip' : ['left_knee', 'right_hip'], 
           'right_hip' : ['right_knee'],
           'left_knee' : ['left_foot'], 
           'right_knee' : ['right_foot'],}



JOINTS_WITH_ORIENTATION = ['head', 'neck', 'torso', 'left_shoulder', 'left_elbow', 
                             'right_shoulder', 'right_elbow', 'left_hip', 'left_knee',
                             'right_hip', 'right_knee']

JOINTS_WITHOUT_ORIENTATION = ['left_hand', 'right_hand', 'left_foot', 'right_foot']

JOINTS = JOINTS_WITH_ORIENTATION + JOINTS_WITHOUT_ORIENTATION


#+end_src

**** Pose data structures
#+begin_src python
import numpy

class Joint:
  position = None;
  orientation = None;
    
  def __str__(self):
    return "Joint[\n Position: %s,\n Orientation:\n %s ]" % (self.position, self.orientation)
      

def parse_joint(data):
  joint = Joint();
  if len(data) > 4:
    joint.position = numpy.array(data[10:13]) / 1000;
    joint.orientation = numpy.array(data[0:9]).reshape((3,3));
  else:
    joint.position = numpy.array(data[0:3]) / 1000;
  return joint
  

class Pose:
  joints = dict();
   
  def __init__(self, data):
    pos = 1;

    for joint_name in JOINTS_WITH_ORIENTATION:
      joint = parse_joint(data[pos:pos+14]);
      pos += 14;
      self.joints[joint_name] = joint;

    for joint_name in JOINTS_WITHOUT_ORIENTATION:
      joint = parse_joint(data[pos:pos+4]);
      pos += 4;
      self.joints[joint_name]  = joint;
#+end_src

**** RVIZ visualization
***** Node setup
#+begin_src python
import roslib;
import rospy;
import math;
from visualization_msgs.msg import Marker
from visualization_msgs.msg import MarkerArray

topic = 'visualization_marker_array'
publisher = rospy.Publisher(topic, MarkerArray)

rospy.init_node('skeleton_pose_visualizer')

#+end_src

#+RESULTS:

***** ROS messages
#+begin_src python
def create_joint_message(joint, id=0):  
  marker = Marker()
  marker.header.frame_id = "/skeleton"
  marker.type = marker.SPHERE
  marker.id = id
  marker.action = marker.ADD
  marker.pose.position.x = joint.position[0]
  marker.pose.position.y = joint.position[1]
  marker.pose.position.z = joint.position[2]
  marker.scale.x = 0.05
  marker.scale.y = 0.05
  marker.scale.z = 0.05
  marker.color.a = 1.0
  marker.color.r = 1.0
  marker.color.g = 1.0
  marker.color.b = 0.0

  return marker

  
from geometry_msgs.msg import Point

def create_link_message(pose, id=0):

  def pos2Point(joint):
    return Point(joint.position[0], joint.position[1], joint.position[2]);

  points = []
  for jointName1 in LINKS.keys():
    for jointName2 in LINKS[jointName1]:
      joint1 = pose.joints[jointName1];
      joint2 = pose.joints[jointName2];
      points.append(pos2Point(joint1));
      points.append(pos2Point(joint2));

  marker = Marker()
  marker.header.frame_id = "/skeleton"
  marker.type = marker.LINE_LIST
  marker.id = id
  marker.action = marker.ADD
  marker.scale.x = 0.02
  marker.color.a = 1.0
  marker.color.r = 1.0
  marker.points = points

  return marker


  
def create_pose_message(pose):
  markerArray = MarkerArray()
  id = 0
  for joint in pose.joints.values():
    markerArray.markers.append(create_joint_message(joint, id))
    id += 1    
    markerArray.markers.append(create_link_message(pose, id))

  return markerArray

#+end_src

#+begin_src python
def visualize_frame(frame, dataset_person=DatasetPerson()):
  publisher.publish(create_pose_message(dataset_person.get_pose(frame)))


import time

def visualize_interval(start_frame=1, end_frame=1000, dataset_person=DatasetPerson()):
  for frame in range(start_frame, end_frame):
    visualize_frame(frame, dataset_person);
    time.sleep(1.0/25.0)
#+end_src

** gplvm
#+begin_src python
import numpy as np
import string
import matplotlib.pyplot as pb
import GPy

def learn_GPLVM(activity):
  p = DatasetPerson();
  p.load_activity(activity);
  data = p.get_processed_data();
  input_dim = 3
  kern = GPy.kern.rbf(input_dim)
  # kern = GPy.kern.periodic_exponential()
  m = GPy.models.BCGPLVM(data, input_dim=input_dim, kernel=kern)

  # initialize noise as 1% of variance in data
  # m['noise'] = m.likelihood.Y.var()/100.
  m.optimize('scg', messages=1, max_iters=1000)

  return m
#+end_src

#+begin_src python
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

def visualize_latent_model(model):
  fig = plt.figure()
  ax = fig.add_subplot(111, projection='3d')

  xs = model.X[:,0]
  ys = model.X[:,1]
  zs = model.X[:,2]
  ax.scatter(xs, ys, zs)

  ax.set_xlabel('latent 1')
  ax.set_ylabel('latent 2')
  ax.set_zlabel('latent 3')

  plt.show()

#+end_src

#+begin_src python
import GPy
#+end_src

** Sandbox
#+begin_src python

#+end_src

* Unsorted                                                         :noexport:
** Links
- [[http://glowingpython.blogspot.de/2012/10/visualizing-correlation-matrices.html][visualizing a correlation matrix]]
** Cites
*** Simplicity
Simplicity is a great virtue but it
requires hard work to achieve it
and education to appreciate it.
And to make matters worse:
complexity sells better.
Edsger Wybe Dijkstra 

Simplicity is the ultimate
sophistication.
Leonardo da Vinci
** Ideas
* Deprecated                                                       :noexport:
** Lisp
*** Configuration
**** Prerequisites
***** Common lisp
- sbcl
- quicklisp
***** System
- ros (hydro)
- gsl library

**** Start roscore
#+begin_src sh :results output :shebang "#!/bin/bash" :session test
 roscore&
#+end_src


**** Common Lisp Initialization
[[http://common-lisp.net/project/asdf/asdf/Configuring-ASDF.html][Configuring ASDF]]

Install all ros related packages. e.g:
#+begin_src sh
 sudo apt-get install ros-hydro-roslisp*
 sudo apt-get install ros-hydro-cl-*
#+end_src


We want to run common lisp ros code outside of catkin.
Add the following two files:

***** ~/.config/common-lisp/source-registry.conf.d/roslisp.conf
#+begin_src lisp
(:tree "/opt/ros/hydro/share/")
#+end_src

***** ~/.config/common-lisp/source-registry.conf.d/msgs.conf
#+begin_src lisp
(:tree "/opt/ros/hydro/share/common-lisp/ros/")
#+end_src

*** Visualization

**** Lisp
***** Common lisp packages Initialization
#+begin_src lisp :session 
  (ql:quickload "cl-ppcre")
  (ql:quickload "gsll")
  (ql:quickload "roslisp")
  (ql:quickload "alexandria")

#+end_src

#+RESULTS:
| alexandria |


#+begin_src lisp  :session :results silent
  ; making sure that roslisp is loaded
  (asdf:operate 'asdf:load-op :roslisp)

  ; making really sure that roslisp is loaded
  (ros-load:load-system :roslisp)
  (ros-load:load-system :cl-transforms)  
  (ros-load:load-system :visualization_msgs-msg)
#+end_src

***** Utils
****** Data set reading utils
#+begin_src lisp :session
  (defun read-file (path)
    (let ((lines (make-array 1 :fill-pointer 0)))
      (with-open-file (stream path)
        (do ((line (read-line stream nil)
                   (read-line stream nil)))
            ((null line))
          (vector-push-extend line lines)))
      lines))
#+end_src

#+RESULTS:
: READ-FILE


#+begin_src lisp :session
(defun read-frame (frame &optional (data *annotations*))
    (mapcar #'read-from-string  (cl-ppcre:split "," (aref data frame))))
#+end_src

#+RESULTS:
: READ-FRAME

****** List -> multidimensional array (matrix)
#+begin_src lisp :session
(defun list->matrix (lst)
           (let ((array (make-array '(3 3))))
             (setf (aref array 0 0) (first lst))
             (setf (aref array 0 1) (second lst))
             (setf (aref array 0 2) (third lst))
             (setf (aref array 1 0) (fourth lst))
             (setf (aref array 1 1) (fifth lst))
             (setf (aref array 1 2) (sixth lst))
             (setf (aref array 2 0) (seventh lst))
             (setf (aref array 2 1) (eighth lst))
             (setf (aref array 2 2) (ninth lst))
             array))
#+end_src

#+RESULTS:
: LIST->MATRIX

***** Data: Joint/Skeleton objects
 #+begin_src lisp  :session
   (defstruct joint
     position 
     orientation)
   
   (defstruct skeleton
     frame
     joints
     links)
   
   (defmacro x-pos (joint)
     `(first (joint-position ,joint)))
   
   (defmacro y-pos (joint)
     `(second (joint-position ,joint)))
   
   (defmacro z-pos (joint)
     `(third (joint-position ,joint)))
#+end_src

 #+RESULTS:
 : Z-POS

***** Function: Parse the data and create a skeleton object

#+begin_src lisp :session 
  
  (defvar *links*  '((torso neck) (torso left_shoulder) (torso right_shoulder)
                     (torso left_hip) (torso right_hip)  (neck head) 
                     (left_shoulder left_elbow) (right_shoulder right_elbow)
                     (right_elbow right_hand) (left_elbow left_hand)
                     (right_shoulder left_shoulder)
                     (left_hip left_knee) (right_hip right_knee)
                     (left_knee left_foot) (right_knee right_foot)
                     (left_hip right_hip)))
  
  (defvar *joints-with-orientation* '(head neck torso left_shoulder left_elbow 
                          right_shoulder right_elbow left_hip left_knee
                          right_hip right_knee))

  (defvar *joints-without-orientation* '(left_hand right_hand left_foot right_foot))

  (defvar *joints* (append *joints-with-orientation* *joints-without-orientation*))

#+end_src

#+RESULTS:
: *JOINTS*


#+begin_src lisp :session 
  (defun create-joint-from-list (lst)
    (make-joint
     :orientation (list->matrix (subseq lst 0 9))
     :position (subseq lst 10 14)))
  
  (defun create-skeleton-from-data (lst)
    (let ((start 0))
      (flet ((next-chunk (size)
               (let ((result (subseq lst start (+ start size))))
                 (setf start (+ start size ))
                 result)))
        (let ((frame (next-chunk 1))
              (joints nil)
              (links *links*))
          (dolist (joint-name *joints-with-orientation*)
            (push (cons joint-name (create-joint-from-list (next-chunk 14))) joints))
          
          (dolist (joint-name *joints-without-orientation*)
            (push (cons joint-name (make-joint :position (next-chunk 4))) joints))
          
          (make-skeleton :frame frame :joints joints :links links)))))  
#+end_src

#+RESULTS:
: CREATE-SKELETON-FROM-DATA

***** Function: create ros messages

#+begin_src lisp  :session
  (defun create-joint-message (joint id)
    (let ((pos (joint-position joint)))
      (roslisp:make-message 
       "visualization_msgs/Marker"
       (stamp header) (roslisp:ros-time)
       (frame_id header) "/skeleton" 
       (id) id
       (type)  (roslisp-msg-protocol:symbol-code
                'visualization_msgs-msg:<marker>
                :sphere)
       (action) (roslisp-msg-protocol:symbol-code
                 'visualization_msgs-msg:<marker>
                 :add)
       (x position pose) (/ (first pos) 1000)
       (y position pose) (/ (second pos) 1000)
       (z position pose) (/ (third pos) 1000)
       (x scale) 0.03
       (y scale) 0.03
       (z scale) 0.03
       (g color) 1.0
       (a color) 1.0
       (lifetime) 100)))
#+end_src

#+RESULTS:
: CREATE-JOINT-MESSAGE

#+begin_src lisp :session
  (defun create-link-list-message (points id)
    (roslisp:make-msg 
     "visualization_msgs/Marker"
     (stamp header) (roslisp:ros-time)
     (frame_id header) "/skeleton" (id) id
     (type)
     (roslisp-msg-protocol:symbol-code
      'visualization_msgs-msg:<marker>
      :line_list)
     (action)
     (roslisp-msg-protocol:symbol-code
      'visualization_msgs-msg:<marker>
      :add)
     (x scale) 0.01
     (r color) 1.0
     (a color) 1.0
     (lifetime) 100
     (points) points))
  
  (defun links->line-points (links joints)
    (let ((points nil))
      (mapcar 
       (lambda (el)
         (let ((p1 (joint-position (cdr (assoc (first el) joints))))
               (p2 (joint-position (cdr (assoc (second el) joints)))))
           (push (roslisp:make-msg "geometry_msgs/Point" 
                                   :x (/ (first p1) 1000)
                                   :y (/ (second p1) 1000)
                                   :z (/ (third p1) 1000)) points)
           (push (roslisp:make-msg "geometry_msgs/Point"
                                   :x (/ (first p2) 1000)
                                   :y (/ (second p2) 1000)
                                   :z (/ (third p2) 1000)) points))) 
       links)
      (map 'vector #'identity points)))
  
#+end_src

#+RESULTS:
: LINKS->LINE-POINTS

#+begin_src lisp :session
      (defun create-skeleton-message (skeleton)
        (let ((index 0) (markers 'nil))
          (mapcar (lambda (el) 
                    (push (create-joint-message (cdr el) index) markers)
                    (incf index))
                  (skeleton-joints skeleton))
          
          (push (create-link-list-message 
                 (links->line-points 
                  (skeleton-links skeleton) 
                  (skeleton-joints skeleton))
                 index) 
                markers)
          (roslisp:make-msg "visualization_msgs/MarkerArray" :markers
                            (map 'vector #'identity markers))))
#+end_src

#+RESULTS:
: CREATE-SKELETON-MESSAGE

***** Visualize a frame

#+begin_src lisp :session
  (defun visualize-frame (frame &optional (data *annotations*) (pub *pub*))
    (roslisp:publish pub 
                     (create-skeleton-message (create-skeleton-from-data (read-frame frame data)))))
#+end_src

#+RESULTS:
: VISUALIZE-FRAME

#+begin_src lisp :session
    (defun visualize-interval (start-frame end-frame &optional (data *annotations*) (pub *pub*) (sleep-time 0.05))
      (loop for frame from start-frame to end-frame do
        (progn
          (visualize-frame frame data pub)
          (sleep sleep-time))))
#+end_src

#+RESULTS:
: VISUALIZE-INTERVAL

**** Lisp: visualization test

#+begin_src lisp :session
  (ROSLISP:START-ROS-NODE "test")
  (defvar *pub* (ROSLISP:ADVERTISE "visualization_marker_array" "visualization_msgs/MarkerArray"))
  (defvar *annotations* (read-file "/work/Data/human_activities/data1/0512164529.txt"))

  (visualize-interval 1 1000)
#+end_src

#+RESULTS:
: NIL

* Footnotes

[fn:1] Human Activity Detection from RGBD Images, Jaeyong Sung, Colin Ponce, Bart Selman, Ashutosh Saxena. In AAAI workshop on Pattern, Activity and Intent Recognition (PAIR), 2011. 
[fn:2] RGB-D Camera-based Daily Living Activity Recognition - Chenyang Zhang, Student Member, IEEE and Yingli Tian, Senior Member, IEEE
 
