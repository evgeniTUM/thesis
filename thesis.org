#+PROPERTY: header-args:lisp :results replace :session
#+PROPERTY: header-args:python :results none :session test :exports none

#+COLUMNS: %25ITEM %TAGS %PRIORITY %TODO

* LaTeX                                                            :noheading:

#+BEGIN_SRC emacs-lisp :exports none
(setenv "PYTHONPATH" (concat (getenv "PYTHONPATH") ":./code/spencer"))
#+END_SRC

#+TITLE: Online activity recognition through kernel methods
#+AUTHOR: Evgeni Pavlidis

#+LaTeX_CLASS: scrbook
#+LaTeX_CLASS_OPTIONS: [11pt,a4paper,bibtotoc,idxtotoc,headsepline,footsepline,footexclude,BCOR12mm,DIV13]
#+LaTeX_CMD: xelatex

# --- Packages
#
#+LaTeX_HEADER: \usepackage{pdfsync}
#+LaTeX_HEADER: \usepackage{scrpage2}

#+LaTeX_HEADER: \usepackage{hyperref}


#+LaTeX_HEADER: \usepackage{palatino}
#+LaTeX_HEADER: \usepackage{pifont}
#+LaTeX_HEADER: \usepackage{rotating}
#+LaTeX_HEADER: \usepackage{float}
#+LaTeX_HEADER: \usepackage[utf8]{inputenc}
#+LaTeX_HEADER: \usepackage{marvosym}

#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amsfonts}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{bm}
#+LaTeX_HEADER: \usepackage{textcomp}

#+LaTeX_HEADER: \usepackage{makeidx}
#+LaTeX_HEADER: \usepackage{subfigure}
#+LaTex_HEADER: \usepackage{graphicx}

#+LaTeX_HEADER: \usepackage{todonotes}
#+LaTeX_HEADER: \usepackage{setspace}


#+LaTeX_HEADER: \usepackage{titlesec}
#+LaTeX_HEADER: \usepackage{emptypage}
#+LaTeX_HEADER: \usepackage{styles/tumlogo}


# --- Options
#
#+LaTeX_HEADER: \pagestyle{scrheadings}



# --- TITLE ---
#
#+LaTeX_HEADER: \let\OldMaketitle\maketitle
#+LaTeX_HEADER: \renewcommand{\maketitle}{
#+LaTeX_HEADER: \pagenumbering{roman} 
#+LaTeX_HEADER:
#+LaTeX_HEADER: }



#+begin_latex
\include{components/info}
\include{components/cover}
\include{components/titlepage}

\include{components/abstract}
\include{components/abstract_german}
\include{components/disclaimer}
#+end_latex






# --- Table of Contents
# 
#+OPTIONS: toc:nil   
#+TOC: headlines 2

# --- Bibliography
#
#+BIBLIOGRAPHY: bibliography plain limit:t
#+STYLE: &lt;link rel="stylesheet" type="text/css" href="css/org.c


#+begin_latex

\newcommand{\TODO}[1]{\todo[color=red]{#1}}
\pagenumbering{arabic} 


\setcounter{secnumdepth}{2}

#+end_latex


\TODO{cite:software packages and tools used}
\TODO{cite:datasets (mocap, daily activities, ms activities)}
\TODO{Check bibliography style and data!!!}
\TODO{define simple variables mathematically eg. feature sequence etc.}

* Introduction
** Motivation
Activity recognition is a big research field in machine learning and robotics. Being able to infer what human actors are doing helps in many practical robotic tasks. An example is doing short-time prediction for collision-avoidance. Moreover in social robotics it is crucial to know what humans are doing when reasoning about the current state of the robot's environment.

To do activity recognition the human pose has to be inferred for each frame.  
With the advent and further development of RGBD (color and depth) sensors it is now possible to perform skeletal tracking of persons. This allows us to decouple pose estimation and activity learning, which make the problem a bit easier.

For real robotic tasks it is very important that the activity recognition is online i.e. runs in real-time. 

Advantages of skeletal features are that no person sensitive information has to be processed by the learning algorithm. In the context of the SPENCER project this is an important prerequisite. Another advantage is also that the skeletal features are very informative for activity recognition. Also beginning to concept an algorithm that builds on top of robust pose estimation reduces the complexity, as we can fully ignore the pose estimation problem.

** Problem statement

\todo{make a distinction between action and activity}
\todo{make a distinction between online recognition and online learning !!! maybe change online to real-time}

The biggest challenge in activity recognition is classification of time series data. In contrast to the simpler sample model, here we have to classify sequences. Therefore appropriate models have to be implemented that take the dynamics into account.

On top of that the problem is ill-posed. 

Also for online activity recognition we have to classify incomplete data consisting of subsequences. This makes the problem more challenging and reduces the pool of methods that can be used for the classification.

We identify three requirements for a practical activity recognition system:
1. Online recognition\\
   The algorithm has to be able to do real-time classification as the sequence is progressing.
2. Classification of incomplete sequences\\
   This follows directly from the first requirement. There should be no assumptions about the activities regarding completeness, length or periodicity.
3. Novelty detection\\
   The algorithm should be able to recognize unobserved activities. 

Optimally it should be possible to also recognize when an unknown activity started and finished. This way automatic segmentation will be possible and will considerably reduce the supervised learning time. \todo{elaborate on this} In combination with active learning this will greatly reduce training time for practical applications.

Nice to have... online learning??

*** Online (active) learning of human activities
Use gaussian processes to learn new activities in real time
*** Evaluate Gaussian processes against different ml algorithms for activity recognition
Evaluate the performance of GPs in relation to the other solutions

*** Scope
*** The SPENCER project
\missingfigure{spencer prototype robot (Bender B21)}

\missingfigure{spencer robot}
With the modern technologies (ms Kinect SDK, primesense ...) it is possible to decouple skeleton tracking and learning

\missingfigure{skeleton representation}

** Prerequisites and notation
We assume a basic understanding in /Linear Algebra/ and /Probability theory/ . Although a high-level overview on machine learning is given in chapter 2 deeper knowledge in this field will help understand the /Related Works/ chapter better.

*** Mathematical notation
- Matrices uppercase
- Vectors lowercase bold
- Constants lowercase
- Parameters lowercase greek letters
** Outline
- Introduction ::
   This chapter introduced the topic of this work. The motivation and the scope is explained.
- Background ::
   The second chapter summarizes some basic concepts and models that are prerequisites for our approach. It begins with an overview of machine learning and introduces the multivariate gaussian distribution. Then an emphasis is led on Gaussian Process Regression and Gaussian Process - Latent Variable Models, which is an unsupervised learning method for dimensionality reduction. Last the Dynamic Time Warping algorithm, which is used for sequence alignment, is explained.
- Related Work :: The third chapter gives an overview of methods used in similar approaches and then analyses strength and weaknesses of these methods in regards to online activity recognition.
- Approach :: The fourth chapter presents two approaches to online activity recognition and their implementations. The first one is an implementation of "Discriminative Sequence Back-constrained {GP}-{LVM} for {MOCAP} based Action Recognition}" ebib:_discriminative_2013. The second one is a novel approach which learns a dense motion flow field in latent space through Gaussian Process Regression.
- Evaluation :: In the fifth chapter the two approaches are being evaluated and discussed. 
- Results and Outlook :: The last chapter summarizes the results of the two approaches and gives a brief outlook of future improvements.

* Background
This chapter introduces some basic concepts needed to understand the proposed approaches. First an high-level overview is given on machine learning and its terminology. After that the gaussian distribution is presented in its univariate and multivariate variants. Following is an explanation of /Gaussian Processes/, their different interpretations and properties. After that the  /Gaussian Process - Latent Variable Model/ is being introduced along with some extensions for learning a backward mapping and optimizing it for discrimination in the case of multi-class classification. Last the /Dynamic Time Warping/ algorithm is presented.
 
** Machine Learning
Machine Learning is a discipline where one tries to learn from data.  
*** Supervised learning
Supervised learning is the task of classification or regression when the data is labeled i.e. we have the ground truth of every sample.
The algorithm then takes the labeled samples (and maybe some confidence values) and infers the model parameters (or hyperparameters) according.

There are two distinct cases in supervised learning:

**** Classification
\missingfigure{classification example}
Classification the task of learning which category a sample belongs to. A prominent example is Spam filtering. By taking a large number of emails which are labeled either as spam or as ham (regular email), the algorithm deduces a model which can classify unknown samples into these two categories.

**** Regression
\missingfigure{regression example}
Regression is a terminus in machine learning and means function approximation. Here the domain of the sample's label is continuous. 
An example would be ...


In most cases we search for a good model that explains the data we have. Parametric models, for example, try to learn the ...
When searching for an appropriate model it is also important that we try to capture the underlying relationship without compromising the generalization property, which is the ability of the model to correctly predict unseen samples. The case that an algorithm learns the relationship of the data that is used to train the model (training data) but poorly predicts new samples is called overfitting. When a model  


Very often the parameter search is done by maximizing the probability of the data given the model parameters. 

$$ \operatorname{arg\,max}_{\bm{\theta}} p(\bm{X} | \bm{\theta}) = \operatorname{arg\,max}_{\bm{\theta}} \frac{p(\bm{\theta}|\bm{X}) * p(\bm{X})}{p(\bm{\theta})} $$

where $\theta$ are the model parameters and $X$ is the data.

*** Unsupervised learning
In contrast to supervised learning in unsupervised learning we have no labeled data i.e. there is no supervisor giving each sample a category (classification) or a value (regression). In this case we can only derive properties of the generation process. Therefore we try to detect patterns in the unlabeled data. These pattern may be clusters of similarity or a lower dimensional generative manifold from which the samples are generated. The last one is called Dimensionality Reduction which will be also a subject in this work. ebib:bishop_pattern_2006 

\missingfigure{dimensionality reduction example}

*** Generative models
Generative methods model the underlying process which generates the data. In Bayesian terms we model the likelihood and the. Thus more data is needed to find an appropriate model. On the other side the model is very flexible and many attributes have a natural interpretation. An example of this is \todo{generative model example}

*** Discriminative models
A discriminative model is only concerned with modeling the actual posterior. This way fewer samples are needed to find the model parameters but by not taking the prior into account the model becomes more generative and is susceptible to overfitting.

*** Online learning
Algorithms which can be gradually optimized towards a good solution using streaming batches of samples are considered to do online learning. In contrast to online learning online recognition means that the algorithm works in real-time and fast recognition is possible.

*** Active learning
Very often the bottleneck of powerful supervised learning techniques is that they rely on correctly labeled data. Since labeling has to be performed by a human it is very difficult and costly to label large amount of data. By identifying more important samples by their entropy, thus information ability of selecting a good model, it is possible to achieve good results with fewer samples.

Letting the algorithm select such samples and query only their labels from a human, who is now actively participating in the learning loop, is called active learning.

Active learning is in practice a convenient way to acquire new informative samples without letting someone go over a huge amount of data to label.

** The gaussian distribution
*** Univariate gaussian distribution
In the one dimensional case the gaussian distribution is well known and understood. Moreover many processes in nature can be modeled with this distribution and for this reason it is also called the Normal distribution. The probability of an event is very high on a certain "point" (its meain value $\mu$) and it drops quickly on each side with the standard deviation $\sigma$.

$$ \mathcal{N}(\mu, \sigma^2) = \frac{1}{\sigma  \sqrt{2 \pi}}e^{-\frac{x-\mu}{2 \sigma^2}} $$

One disadvantage of this distribution which we can see from the above formula is that it can model only one hypothesis. This is also the case for the gaussian distributions of multiple (multivariat gaussian distribution) and infinite (gaussian process) dimensions.

*** Multivariate gaussian distribution
The multivariat gaussian distribution is the generalization of the gaussian distribution in higher dimensions.

$$ \mathcal{N}(\bm{\mu}, \bm{\Sigma}) =  \frac{1}{  \sqrt{(2 \pi)^d |\bm{\Sigma}|}}
e^{-\frac{1}{2} (\bm{x} - \bm{\mu})^T \bm{\Sigma}^{-1}  (\bm{x} - \bm{\mu})} $$

The two parameters of the distribution are:
- mean :: $\bm{\mu} = E[x]$ Representing the most probable vector
- covariance :: $\bm{\Sigma}$ Representing the mutual variance for each pair of the elements of the random vector: $\bm{\Sigma}_{ij} = Cov[x_i, x_j]$

The exponent is mahalanobis distance, which measures the distance of a point to the ellipsoid defined by the covariance matrix.\todo{cite}

*** Properties of gaussian distributions
\todo{Write about total probability and such}
Aside for being an appropriate model for many processes occurring in nature, gaussian distributions are also very nice to work with. The marginal and conditional of two gaussian distributions are also gaussian. 

One reason GPs are straightforward and work is the math behind them. It is just linear algebra operations.

Linear maps for gaussian distributions:

Product of two multivariate gaussian distributions:

$$ \mathcal{N}(\bm{x}; \bm{\mu_x}, \bm{\Sigma_x} ) \mathcal{N}(\bm{y}; \bm{\mu_y}, \bm{\Sigma_y} ) =  
\frac{1}{  \sqrt{(2 \pi)^d |\bm{\Sigma}|}} e^{-\frac{1}{2} (\bm{x} - \bm{\mu_x})^T \bm{\Sigma}^{-1}  (\bm{x} - \bm{\mu})} 
\frac{1}{  \sqrt{(2 \pi)^d |\bm{\Sigma}|}} e^{-\frac{1}{2} (\bm{x} - \bm{\mu_y})^T \bm{\Sigma}^{-1}  (\bm{x} - \bm{\mu_y})} $$


Marginal of a multivariate gaussian:

Conditional of a multivariate gaussian: 



\todo{cite Gaussian Winter School slides Philipp Hennig}

** Gaussian Process - Latent Variable Model

The GP-LVM is an unsupervised learning model to perform a non-linear dimensionality reduction from an observed space$X$ to a latent space $Y$ 
It does this by maximizing the likelihood $$p(Y|X) = p(Y|f)p(f|X)$$ using a gaussian prior for the mapping $f$. Technically a GP-LVM is a product of Gaussian Processes which model a regression of the mapping from latent space to observed space. \todo{formulas etc.} This means also that if we want to compute the latent position of a new observed sample we have to compute the ...\todo{elaborate GP-LVM}. Using a linear kernel the model generalizes to \todo{PCA} PCA. By using a non linear kernel a non-linear mapping is inferred making it a non-linear latent variable model.ebib:lawrence_probabilistic_2005

#+CAPTION: [GP-LVM example]{GP-LVM example: Human pose reduction (walking activity)}
\missingfigure{example GP-LVM, skeleton}

Analogy LVM <-> marionettes

*** Principal Components Analysis
The Principal Components Analysis is an method which.

This method has an 
Tipping and Bishop, Journal of the Royal Statistical Society (1999)
  
*** Back-constraints GP-LVM
One problem with this model is that it does not preserve local distances in the latent space. This is because it tries to explain the data by moving distant samples from the observed space also far apart in the latent space. This problem is addressed by Lawrence et al. in the back-constrained GP-LVM ebib:lawrence_local_2006. A mapping $g_i(y_i) = x_i$ is introduced which constrains the points in latent space to be more near if they are also near in observed space. Instead of optimizing directly on $X$ the back-constrained GP-LVM optimizes the mapping $X=f(Y)$ instead. This back-constrained mapping 

Having this back-constraints also gives us a mapping from observed space to latent space which can be used to project a new sample into the latent space without costly maximum likelihood estimates. 
\missingfigure{example BCGPLVM}

*** Discriminative GP-LVM
Another improvement in the context of classification in latent space is the Discriminative GP-LVM ebib:urtasun_discriminative_2007. Using a /General Discriminant Analysis/ criterion a prior is being enforced on the latent space which ensures that samples from one class are more clustered and different classes are more separated. This is done by maximizing the between-class separability and minimizing the within-class variability while optimizing the log likelihood of the GP-LVM.ebib:urtasun_discriminative_2007
 
*** Locally-Linear GP-LVM
ebib:urtasun_modeling_2007-1
*** Other variants
**** Bayesian GP-LVM
An interesting approach for computing the likelihood of the latent variable mapping was proposed in ebib:titsias_bayesian_2010. By using a variational method it becomes possible to marginalize over $X$. Doing so the mapping can be learned together with an \todo{explain ARD} ARD kernel. This way the dimensionality of the manifold can be learned from the data. 

**** Subspace GP-LVM

**** Manifold Relevance Determination
Combining the Subspace GP-LVM with the variational approach and the ARD kernel it is possible to learn the manifold \todo{explain MRD}.ebib:damianou_manifold_2012

*** Advantages
- probabalistic
- Generative: it can generalize beyond training data
- non-linear mapping:
*** Disadvantages
**** No mapping from observation space to latent space
The idea of the GP-LVM is to learn a mapping from latent space to observation space by marginalization over the latent space. Resulting from this is that we do not have an inverse mapping into the latent space. This fact may be of no importance for character modeling and motion interpolation but in our case it is crucial. An inverse mapping can be computed by using the Back-constrained GP-LVM described above. However one should also keep in mind that using back-constraints inherently changes the latent space as employs an additional constraint on the mapping.

**** Very hard optimization problem
Resulting from the disadvantages of Gaussian Process regarding the optimization of the hyper-parameters the GP-LVM is also very hard to optimize as its objective function is non-convex. But in the case of GP-LVM we have a much larger optimization space due to the fact the we do not optimize only the hyper-parameters, of the mapping Gaussian Process, but also the latent space itself which is of dimenionality $n$. 

This in fact is the biggest problem as it limits its use on real world data, because for more complex manifold structures there will likely be many local minima. For this reason it is crucial to choose a good initialization. Examples are PCA, Local Linear Embedding or ISOMAP.
*** GP-LVM for human motion
As the space of human motion is high-dimensional (spatio-temporal) dimensionality reduction is crucial for a number of models dealing with human motion (e.g. ebib:fan_gaussian_2011l).
The GP-LVM preserve the distances in the mapping and are therefore suitable to model human motion with high noise of the poses see Urtasun DGPLVM
Newest addition is ebib:jiang_modeling_2014
*** Initialization
The author in Exploring model selection techniques for
nonlinear dimensionality reduction
also suggest to use ISOMAP or LLE to initialize the GPLVM and argues that direct optimization of the GP-LVM is very difficult.
** Dynamic Time Warping
The Dynamic Time Warping is an algorithm which tries to find a minimal warping path between two sequences. The sequences can be of arbitrary length.
Since we are not interested in the path itself but in the cost of the minimal path we define the DTW as a mapping from two time series to an real value.  

The recursive definition -- excluding some corner cases -- reveals the workings of this method.

#+begin_latex
$$
\text{dtw}_{x,y}(i, j) = \text{dist}(x_i, y_j) + \text{min}
\begin{cases}
   \text{dtw}_{x,y}(i-1, j) \\
   \text{dtw}_{x,y}(i, j-1) \\
   \text{dtw}_{x,y}(i-1,j-1) 
\end{cases}
$$
#+end_latex

Where $\text{dist}(x,y)$ is a distance function which tells how close two points are, and $i$ and $j$ are the element indices for the first and second sequence.
The DTW can be computed with dynamic programming and has a runtime complexity of $\mathcal{O}(n m)$ where $n,m$ are the lengths of the two sequences.

It is closely relates to the /Longest Common Subsequence/ where, instead of minimizing the total warping cost we maximize a common subsequence which is contained in both sequences.
 
We consider DTW to be a distance which is not entirely correct as the triangle inequality does not hold. Nevertheless it gives us a notion of how similar two time series are and since it is non-negative ( $d(x,y) >= 0$ ), symmetric ( $d(x,y) = d(y,x)$ ) and respects the identity property ( $d(x,x) = 0$ ) it can be used to define a meaningful, be it not formally correct, kernel. ebib:shimodaira_dynamic_2001



* Related work
This chapter will introduce some models and their corresponding algorithms for activity recognition. An emphasis is led on methods which work with skeleton data. In the last part a short analysis is done on these methods and some observations are discussed.

** Overview
Activity recognition is a difficult task as we have to make sure our algorithm will discriminate between different classes -- activities -- but also will leave room for inner class variations. These variations are the result of different persons performing activities differently. A simple example is walking, where different person has a different walking style -- also called gait. Also different environments will result in actions to be performed slightly differently. ebib:poppe_survey_2010

There are many methods which learn from videos and try to explain. This approach is very flexible but also has several drawbacks. One of which is that it is very hard to achieve scale and view-invariance. Furthermore inferring the human pose is very difficult and ambiguous. 

For these reasons we will consider only data with pose information in this thesis.
*** machine vision for human activities: a survey ebib:turaga_machine_2008


Generative models such as HMM
Discriminative models such as CRF


Survey on Time-Series Data for classification
** Histogram based approaches
*** Motion history image 
*** Motion energy image
** Dynamic time warping

** A class of space-varying parametric motion fields for human activity recognition

** Action Recognition Based on A Bag of 3D Points
action graph - nodes are shared poses 
** Methods using skeleton features
*** Gaussian Mixture Based HMM for Human DailyActivity Recognition Using 3D Skeleton Features
*** Sung et al. ebib:sung_unstructured_2012
**** Features: Skeleton data + HOG features of RGBD image and depth image 
**** Naive classification: SVM
**** Maximum entropy markov model
Solved via max-flow/min-cut
*** RGB-D Camera-based Daily Living Activity Recognition ebib:zhang_rgb-d_2012
**** Bag of Features
#+begin_src dot :file figures/bag-of-features-approach.png
   digraph pipeline {
     label="pipeline";
     rankdir=LR;

     node [color=blue, shape=box];
  
     feature_extraction;
     k_means;
     vector_quantization;
     centroids[shape=ellipse];

     feature_extraction -> k_means -> vector_quantization -> centroids;

     subgraph {
        label =  "bag_of_features";
     }     
  }
#+end_src

#+RESULTS:
[[file:figures/bag-of-features-approach.png]]

See [fn:2]

**** Features: Structural and Spatial motion
Feature capturing transition between two frames
**** Bag of Features approach (historgram of features)
**** Other: People identification (reidentification)
*** View Invariant Human Action Recognition Using Histograms of 3D Joints
*** Learning Human Activities and Object Affordances from RGB-D Videos 
**** Learning both: activities and object detection/affordance
**** Using Markov Random Field and SVM for learing
*** Eigenjoints ebib:yang_effective_2013
*** Gaussian Process - Latent Conditional Random Field (GP-L CFR)
ebib:jiang_modeling_2014 use GP-LVM to reduce dimensionality of human motion. (earlier approach was Gibbs sampling)
*** Modeling Human Locomotion with Topologically Constrained Latent Variable Models
*** GPDM
In ebib:wang_gaussian_2005 the dynamics of the latent space is being modeled from time series data. In ebib:wang_gaussian_2008 this model is being used to model human motion by applying a GP-LVM to the high-dimensional mocap data and simultaneously learning the dynamic transition in the latent space:

                     $$ x_{t_{k+1}} = f(x_{k}) $$

$f(x)$ is being modeled by a gaussian process.

This model was applied for activity recognition in ebib:jamalifar_3d_2012 where the classification is done through an SVM in the hyperparameter space. (only 2? features)

*** Joint Gait Pose Manifold
The Joint Gait Pose Manifold models the activity and the gait in an common latent space. This way several samples from different persons are modeled with the addition of the gait and do not corrupt the class learning. Each activity is mapped to an toroidal structure where the length represents the activity dynamics and the width represents the gait variation. 

*** Human Action Recognition Using a Temporal Hierarchy of Covariance Descriptors on 3D Joint Locations
** Analysis
Skeleton features are sufficient but other features can be useful:
- hand 
- head pose recognition
- situation awareness
  ...
  
*** Observations
- One observation one can make is that activities are represented by the dynamics of the poses, and thus we try to capture this dynamic model. Several options exist. One way is to use popular graph based probability models, such as Hidden Markov Models, Conditional Random Fields or Actiong Graphs \todo{cite action graph}. Another option is to try to capture the dynamics by appropriate feature extraction. 
  
- Difference between activity and action
  Activities are composed of actions
- Context information can tremendously help in classification of activities (e.g. object detection and human anticipation)
- Skeleton data is sufficient for classification (ebib:ibbt_does_????)
  and also robust to changes in appearance (most state-of-the-art methods work with visual features)
  and also unobtrusive and sensible data doesn't need to be stored (like face features etc.)
- hierarchical learning:
  Some methods learn the actions that a activity is composed of. This practice is also very common in HMM models as they model discrete states and their temporal dependencies
- DTW is a good measure but has several drawbacks, such as in cyclic activities where some motions can be repeated several times
- LLE is not generative therefore LL GP-LVM to preserve smooth map also in latent space

*** Approaches
**** Discriminative Sequence BCGPLVM
Use this to find the activity
***** DTW between walking and walking backwards very big ...
***** not taking temporal dimension into account
**** GPDM
***** approach to classify by hyperparameters not optimal
**** Classify by dynamics of the skeleton (this should bring good classification)
***** GPDM can model the dynamics of the movement
***** has good properties (gaussian processes)
***** has intrinsic dim reduction
***** ?? shared GP-LVM to model different activities in the same latent manifold ??
*** Problems and solutions
**** limited sample data - probabilistic model + discriminative
Probabilistic (and generative ??) models are more accurate using fewer samples, because they model the probability directly ...  
**** high dimensional - dim reduction(gp-lvm)

**** classification - BC GP-LVM + discriminative
**** time series data - GPDM
An can be modeled as a sequence of consecutive poses. Hence a dynamical model. By using a dynamical model classification becomes more discriminative. 
**** confidence is important !!!
Using a probabilistic model (especially gaussian processes) we also get a confidence which in turn can be used for active learning
**** high dim. noise => GP-LVM is very robust because of the nature of optimization (distance is preserved instead of locality)
*** Assumtpions
**** Skeleton tracking is correct and stable
For the algorithm we assume that the skeleton extraction from RGBD data works as expected.
This is far from the truth with current skeleton tracking algorithms but we also get confidences of the poses.
This way we can prune a large number of incorrect poses and because we model the dynamics and do not compare poses this is not a big problem.
**** Smooth skeleton transition !!!
**** Correctly labeled samples (no outliers)
*** Ideas
**** Use hand and/or head features
***** Head direction is important
***** Hand structure is very important for most tasks
***** Object interrelation ???
***** Use HOG for hand features only

**** bag of features 
- no time dependency
- no online capable because of k-means clustering



* Approach
In this chapter two main approaches are presented to do online activity recognition. The first is an extended version of the *Discriminative Sequence Back-constrained GP-LVM* method. Two improvements are proposed. The first one is using also the velocities of the poses for the dimensionality reduction. The second one is a more discriminative Kernel alignment by using the Mahalanobis distance for the DTW. The second approach is a novel method which models the motion flow field inside latent space to capture the dynamics of each activity, named *GP-Latent Motion Flow*. Along with this model we propose an /energy minimization/ approach for online activity recognition which also tries to classify several activities inside a time interval.

** Feature extraction
Regardless of the chosen algorithm the features used for learning will have a big impact on the performance of the model. Therefore it is imperative to extract discriminative 

We get the joint positions and the angles between them in the camera frame defined by the used depth camera (.e.g Microsoft's Kinect). We want these data in the frame of the skeleton. When extracting features we have to make sure that we have view invariant features of the skeleton.

One way to achieve scale invariance is to normalize all link lengths in respect to the torso link. To make the pose view invariant we have to define a local skeleton frame which captures the skeletons /orientation/ in the world coordinate system.

\missingfigure{local skeleton frame}

Another way to achieve view invariance is to not consider the 3D points of the joints all together but instead to take only relative features. These can be, for example the angles or distances between two adjacent joints.

An interesting approach is used in ebib:theodorakopoulos_pose-based_2014, which is to define a polar coordinate frame for each joint and use the two angles as features. This way we also reduce the observation space.

As discussed in [[Related Work]] many methods also make the extracted temporal features (e.g. Eigenjoints). However since we want to include the dynamics in our model we do not extract such features explicitly.

At first we concentrated our efforts for learning with the MOCAP data. In theory the data collected from the kinect should be equivalent. One difference is the high noise in the pose estimation, but due to the fact that the GP-LVM preserves distances rather than locality this problem is mitigated to a certain degree.

** Dynamic time warping with mahalanobis distance 
The Dynamic Time Warping algorithm is a prominent and very effective choice for computing similarity between two sequences. The problem with this approach, in the context of activity recognition, is how to define the distance metric between two poses.

Popular choices for the distance function is the euclidean distance, if 3D points are used as features, and the geodesic distance for angles. The problem with these two distances is that they are just the sum of the individual feature differences. As the dimensionality grows this metric becomes less informative. 

In the case of human poses we have a certain notion of which poses are similar and which are far apart. Maybe this is due to the fact that we inherently know -- or classify -- to which activity the pose corresponds to and have therefore some notion of closeness with respect to an activity which cannot be approximated with the euclidean distance. Poses from different activities will most likely also seem to be more or less similar depending how similar the actions are.

One idea to transfer this knowledge is by using the Mahalanobis distance instead of the euclidean distance when computing the similarity of two pose sequences. By computing the covariance for each activity we have some notion of the variance across all feature dimensions for a specific class. This way we can capture -- to some extent -- the variability for each class. Now we can compute a similarity measure with a new sequence $x_new$ for each class and each sample of this class. Thus we can define a notion of measure between a class and a new sample by:

$$ s(j, \bm{x_{new}}) = \frac{1}{|C_j|} \sum_{\bm{x} \in C_j} \frac{\text{DTW}_{\text{mahalanobis}(\bm{\Sigma_j^{-1}})}(\bm{x}, \bm{x_{new}})}
{min(|\bm{x_i}|, |\bm{x_{new}|)}} $$

where $C_j$ is the set containing all class sequences and $|C_j|$ is the number of sequences in class $j$. The normalization factor $min(|\bm{x_i}|, |\bm{x_new}|)$ makes sure that the minimum cost computed by the $\text{DTW}$ is proportional to the smallest sequence.

This way the distance error is distributed by a way defined by the variance across each dimension.

A similar idea was also proposed in the context of handwritten signature verification in ebib:qiao_learning_2011, which uses just one covariance matrix.
The covariance matrix is determined such that, just like in the case of Discriminant GP-LVM, it maximizes the variability between classes and minimizes the difference for samples in the same class. 
In contrast to our approach the overall covariance matrix may define a more meaningful and discriminative measure but it is also more difficult to update when performing online learning and when learning a new class (novelty detection).

*** Implementation
We wrote a simple version of the Dynamic Time Warping in Python using dynamic programming and following the recursive definition in chapter [[Dynamic Time Warping]]. As the variance for some feature dimensions can be zero the constructed covariance matrix does not have full rank and thus cannot be inverted. We mitigate this problem with an approximation of the inverse by computing the pseudoinverse.

** Discriminative Sequence Back-Constrained GP-LVM
In the paper "Discriminative Sequence Back-Constrained GP-LVM for MOCAP Based
Action Recognition"ebib:_discriminative_2013 the authors propose a method for
classifying MOCAP actions.

#+begin_src dot :file figures/seq-gplvm-approach.png
   digraph pipeline {
     label="Pipeline: Sequence back-constrained GP-LVM pipeline ... CITATION";

     node [color=blue, shape=box];

     subgraph clusterLearning {
        style = filled;
        label =  "learning";
        feature_extraction -> gplvm -> latent_space -> centroids;
        sequence_constraints -> gplvm;
        discriminative_constraints -> gplvm;

        discriminative_constraints [shape=ellipse, label="discriminative  constraints"];
        sequence_constraints [shape=ellipse, label="sequence constraints"];
        { rank=same; gplvm; sequence_constraints; discriminative_constraints; }
     }

     centroids -> SVM;

     subgraph clusterRecognition {
              label = "recognition";
              sequence_mapping -> SVM -> activity_class;           
     }
  }
#+end_src

#+RESULTS:
[[file:figures/seq-gplvm-approach.png]]


By using a similarity feature for the sequences in
the observed space and constraining the optimization to preserve this measure
the local distances between the sequences are transferred into the latent space.
This has two advantages. First of all the sequences have a meaningful clustering
in the latent space. Second by also learning the back-constraint it is possible
to calculate the centroid of a sequence in the latent space directly without
maximizing a likelihood. This in turn is being used to do real-time
classification for actions. The mapping is defined as a linear combination of
the DTW distance between every other sequence. For every latent dimension $q$ we
have:

              $$ g_{q}(Y_s) = \sum_{m=1}^{S} a_{mq} k(Y_s,Y_m) $$

where the similarity measure is $k(Y_s, Y_m) = \gamma e^{DTW(Y_s, Y_m)}$. This
measure is to be preserved in the latent spaces.

       $$ g_q(Y_s) = \mu_{sq} = \frac{1}{L_s} \sum_{n \in J_s} x_{nq} $$

This constraints are being enforced in the optimization by adding Lagrangians to the objective function.

\missingfigure{example of discriminative and back-constrained latent space}

Furthermore, by applying the Discriminative GP-LVM we ensure that poses of different activities are separated from each other and poses from similar activities are located closer together. This ensures that the centroid of an activity is more informative and thus discriminative. The Discriminative GP-LVM works by also maximizing the between class variance and minimizing the in-class similarity ebib:urtasun_discriminative_2007 \todo{expain D GP-LVM properly}
Also by applying the Discriminative GP-LVM the clustering of similar actions and
the distances of different actions is enhanced which allows for a better
classification. Recognition is being done by applying the mapping above to the
new sequence and using a SVM in the latent space.

*** Advantages
Recognition can be done in real time by using the learned back constrained. The centroid in the latent space is being calculated for the whole sequence and classified by the SVM. 
Also incomplete trajectories can be classified.

*** Shortcomings
As the optimization for GP-LVM is determined by the above similarity measure and the discriminative criterion online optimization is very difficult. It is thus highly likely that performing a gradient online optimization will be stuck in an local minimum.

Also one problem with the real-time recognition is that determining when a activity has ended/begun is very difficult. Also as we do not know how long a sequence is we have to calculate the centroid for several time frames.
*** Implementation
As there was no publicly available source code we choose to re-implement this method. As it was planned to implement a /ROS (Robot Operating System)/ module for online activity recognition we choose the Python platform which can be easily integrate with /ROS/. We used the /GPy/ library from the ... Sheffield University \todo{cite GPy}. We ported the Discriminative GP-LVM constraints code from Prof. Urtasun and integrated it with /GPy/. To implement the sequence back-constraints we performed a constrained optimization using Lagrangians.

*** Extensions:
**** Learn poselets (pose and velocities) to capture dynamics
The GP-LVM learns a mapping for each pose but does not consider velocities and accelerations. If we take a pose along with its first and second moments (let us call them poselets) as the high-dimensional space we allow for the temporal displacements to be also modeled.
The latent space represents the poselet and the DTW kernel in the constraint captures also the motion of the activity.
**** Use mahalanobis for the DTW 
** GP-Latent Motion Flow Field (based on the gp regreesion flow)
Many models which use GP-LVM to reduce the high dimensional space into fewer dimension. These approaches make the problem more feasible but the problem remains how to do classification for time-series data. Human motions are mostly characterized by the dynamics of the model (temporal dimension). So we have to compare trajectories in the latent space. One idea is to use GPRF as classification can be done using second order dynamics which should give better results. Going further the activity itself is characterized by the first and second moments of the trajectory function. By explicitly modeling the velocity of the trajectory we can take changes in the joint movement into account.
*** 
The Gaussian Process Regression Flow ebib:kim_gaussian_2011 can be used to model the trajectories in the latent space.

\todo{explain GPRF}

*** GP-Latent Motion Flow
The GP-LMF method is inspired by this model. The difference being that in the case of activity recognition we do not know the starting position and also the trajectories can have significantly different lengths. For this reason it is very difficult to normalize with respect to the time dimension.
Nevertheless, resulting from the properties of Gaussian Process regression, we have also a dense mean flow field and dense variances. This allows us perform efficient and robust online recognition in the latent space.

This model is attractive for two reasons. First real-time classification of incomplete trajectories is possible. Incomplete not only in the sense of the first part of an activity but any interval of an activity, which could be also somewhere in the middle of the sequence. Second it is possible to do online learning by simply adding the new class as a new flow field to the pool of GPs. It is very difficult to adjust the other models for online learning, because of the problem that we can get stuck in a local minimum when optimizing the parameters of the GP.

The idea is to learn a motion field in the latent space for each activity. This can be achieved by learning the velocity function of the latent point just like in the GPRF model presented above. With the difference that we do not use the spatio-temporal domain but spatial domain of the latent space. The reason being that we do not have starting and ending positions for each activity and also the lengths can be variable. On top of that we also want to recognize an activity which is being interrupted by another activity, so we can't fix the lengths of the trajectories. 

\missingfigure{example of several flow fields inside latent space}

Each activity has its own flow field. Recognition and prediction is done by calculating the energy of the currently moving point with each different field. The field with the minimum energy represents the most probable activity as the point follows more closely its "current" of motion.

Variances in the speed of performing an activity can be modeled by giving the point in the latent space a mass which can be adjusted in real time.
When a point has greater mass then it needs more energy to be propagated through the flow field (the overall activity is slower) and vice versa.

An advantage of this method is that activities with repetitive motions, such as walking or running, can be learned without using periodic kernels or other means to model them explicitly. Repetitive motions can be seen as just multiple samples of the same motion which define the flow field.

#+begin_src dot :file figures/gplmf-approach.png
digraph pipeline {
        label="Pipeline: Gaussian Process - Latent Motion Flow";

        node [color=blue, shape=box];

        subgraph clusterLearning {
                label = "learning"
        
                subgraph clusterDimReduction {
                        style = filled;
                        label =  "dim. reduction";
                        feature_extraction -> gplvm -> latent_space;
                        back_constraints -> gplvm;

                        back_constraints [shape=ellipse, label="back constraints"];
                        { rank=same; gplvm; back_constraints; }
                }

                latent_space -> numerical_derivative -> GPs -> flow_model;
                
                
        }

        energy_computation -> flow_model [arrowhead=dot, style=dashed];

        subgraph clusterRecognition {
                label = "recognition";
                online_sequence -> energy_computation -> class;           
        }
}
#+end_src

#+RESULTS:
[[file:figures/gplmf-approach.png]]

*** Learning the flow field

The initial idea was to learn a general dimensionality reduction for a high number of varying activities and work with only one latent space. The problem is that the it is very difficult to learn a smooth mapping in the latent space. This is described more deeply in ebib:urtasun_modeling_2007-1 where the authors try to incorporate the optimization criterion of Locally Linear Embedding together with the a back-constrained Gaussian Process Dynamical Model. As this approach needs also prior knowledge and is very complex we decided to learn each activity separately. Future work should deal with the possibilities of learning a unified latent space at it will allow us to learn different flow fields in the same space and we will not have to perform a heuristic normalization.
 
We deploy GP for learning the flow field which gives us several advantages.

\missingfigure{latent space (several samples of one activity) with flow field}
**** Effects of the hyperparameters

Changing the /lengthscale/ defines how much each point is contributing to the regression process. It can be interpreted as a smoothness factor which governs how strong the interpolation of the flow field is performed on the latent points.

Changing the signal variance controls how much 


\missingfigure{effect of hyperparameters on the resulting flow field}



*** Interpretation
The proposed model has a natural interpretation. A point represents a pose in latent space and an activity is a trajectory in time inside the same space. With the flow field we learn the motion tendencies for each pose. When performing recognition we let the current point traverse each separate flow and compute the needed energy. If we consider that the point has a mass we can model the speed at which activities are being done. This way we can recognize when a point leaves an activity, which represents a /motion current/, and passes over to some other activity.

The model captures the changes in velocity which is comparable to the motion history images...


*** Advantages
**** Recognition
The current activity is being mapped into the latent space. Through the learned back-constrained. The recognition is being performed solely in the latent space. By propagating the current position by each flow field we can calculate the next possible pose. By comparing the similarity considering the variances we have a measure of how well the current activity resamples each flow field e.g. learned activity.

**** Prediction
If we have detected the activity predicting is simply a matter of propagating the pose through the flow field by taking the mean of the GP.

**** Online learning

**** Natural interpretation
**** Novelty detection (anomaly detection)
In ebib:kim_gaussian_2011 the authors present the ability of the GPRF model for anomaly detection. 
This approach is also suitable for finding new classes as the above energy value can be used to recognize novel activities. The reasoning is that if we cannot find an flow field with a small energy the activity has to be unobserved.

**** Active learning
**** Multiple Hypothesis Prediction
Since we have a GP representing our flow field we can predict future point positions with the mean value. Moreover also having informative variances we can sample several possible trajectories. This can be accomplished using an particle filter. Hence we can have multi-hypothesis predictions along with their probabilities.
**** In comparison to the GPDM it can model cyclic activities
*** Problems
**** Dimensionality reduction
Performing a non-linear dimensionality reduction is no easy task. Testing was done with only two dimensions as it easier to visualize the latent space and the resulting flow fields.
A latent space with higher dimension will naturally make the reduction more robust and the field will have a more natural interpretation....

**** Stable class mean flow field
When learning a stable flow field from several samples the field can degenerate with the inclusion of strong variable paths. Therefore it is important to ensure that the algorithm learns stable paths. This can be achieved by sampling uniform random sampling from all samples of the same activity.

\todo{active learning - problem ??}

*** Learning the motion flow field
One problem we encounter by learning the motion flow field from several samples is complexity of the Gaussian Process. There are two solutions for this. The first one is to use a sparse GP model. The second one is to sample points from all samples and use only those that are most suitable for the regression. If we take IVM as the sparse GP model both approaches can be seen as equivalent as the IVM will automatically take the most informative samples.

*** Recognition
Energy minimization:

$$ E_{t_1,t_n} = \sum_{i \in \mathcal{T}, j \in \mathcal{T}} E_{i,j} + regularizer $$

where the regularizer ensures that we do not change flow fields often...

** Bag-of-features

* Evaluation
** Datasets
*** [[http://pr.cs.cornell.edu/humanactivities/data.php][Cornell Activity Dataset]]

\missingfigure{sample images from the dataset}
Active learning using Gaussian Processes.
We will use the "Cornell Activity Datasets (CAD-60 & CAD-120)"[fn:1] to learn and evaluate 
the performance of an implementation of Gaussian Processes. 

The data set s consist of an sequence of frames which include: 
- Image data
- RGBD data
- Skeleton information: (joint position and orientation)
- annotated meta information (e.g. activity)
** Software
MATLAB - FGPLVM 
Dataset: [[http://mocap.cs.cmu.edu][CMU Motion capture dataset]]
- Emacs/Org-mode
- IPython
- SciPy/NumPy
- GPy
- mlpy

** Mahalanobis DTW

** Discriminative Sequence back-constrained GP-LVM
As the idea was to implement the algorithm in an language that can be easily integrated into the ROS infrastructure we implemented the model in Python.
Unfortunately we were not able to perform an appropriate dimensionality reduction. We believe that the many constraints on the optimization and the highly different data is very hard to optimize. For this reason we choose to implement a new model basing on motion flow fields.
** Gaussian Process - Latent Motion Field





* Conclusions and Outlook
** Summary
*** Dimensionality reduction for all activities is very difficult (also with extra constraints)
*** Dynamics is a good measure for classification of human activities
*** Contributions
**** Advantages and Disadvantages of dimensionality reduction with GP-LVM for human motion in the context of activity recognition
**** Implementation of the Discriminative GP-LVM with python 
We ported the matlab code provided by Prof. Urtasun into python and integrated it with the GPy library
**** Implementation of the Sequence Back-constraints 
We used Lagrangians to implement a constrained optimization of the likelihood function
**** Improvement of the DTW measure with the mahalanobis distance ????????
**** A novel approach for activity recognition (prediction??)
**** Introduction of an energy minimization approach for online recognition of complex activities
** Outlook
*** Energy minimization evaluation
*** Semi-supervised activity learning by automatic  segmentation of activities !!!




* Latex end                                                        :noheading:
#+begin_latex
\listoffigures
\bibliographystyle{plain}
\bibliography{bibliography}
#+end_latex


* LAB                                                              :noexport:
** Classification
*** Dataset management
#+begin_src python
import glob
import os
import numpy as np


data_set_indices = []
# indices of positions of first 11 joints (joints with orientation)
# 9 ori + 1 conf   +   3 pos + 1 conf = 14 
for joint in range(0,11):
  for x in range(10,13):
    data_set_indices.append(1 + joint*14 + x);

# indices of hands and feet (no orientation)
for joint in range(0,4):
  for x in range(0,3):
    data_set_indices.append(155 + joint*4 + x);
        

default_data_dir=os.getenv("HOME")+'/data/human_activities'

      
class DatasetPerson:

  data_dir = "";
  person = -1;
  direcotory = "";
  activity_label = dict();
  classes = list();
  activity = ''
  data = None

  def __init__(self, data_dir=default_data_dir, person=1):
    self.data_dir = data_dir;
    self.person = person;
    self.directory = data_dir + '/data'+ str(person) + '/';

    # read labels
    with open(self.directory + '/activityLabel.txt') as f:
      self.activity_label = dict([filter(None, x.rstrip().split(',')) for x in f if x != 'END\n']);

    self.classes = list(set(self.activity_label.values()));
    self.activity = self.activity_label.keys()[0]
    self.load_activity(self.activity)


  def load_activity(self, activity):
    self.activity = activity
    file_name = self.directory + activity + '.txt';
    self.data = np.genfromtxt(file_name, delimiter=',', skip_footer=1);

  def get_processed_data(self):
    data = self.data[:, data_set_indices];

    # take relative position of the joints (rel. to torso)
    for row in data:
      torso_position = row[6:9]
      for joint in range(0, 15):
        row[joint*3:joint*3+3] -= torso_position

    return data

  def get_pose(self, frame):
    return Pose(self.data[frame])
#+end_src

*** Visualization
**** Skeleton structure
#+begin_src python
LINKS = {'torso' : ['neck', 'left_shoulder', 'right_shoulder', 'left_hip', 'right_hip'],
         'neck' : ['head'], 
         'left_shoulder' : ['left_elbow'],
         'right_shoulder' : ['right_elbow', 'left_shoulder'],
           'right_elbow' : ['right_hand'], 
           'left_elbow' : ['left_hand'], 
           'left_hip' : ['left_knee', 'right_hip'], 
           'right_hip' : ['right_knee'],
           'left_knee' : ['left_foot'], 
           'right_knee' : ['right_foot'],}



JOINTS_WITH_ORIENTATION = ['head', 'neck', 'torso', 'left_shoulder', 'left_elbow', 
                             'right_shoulder', 'right_elbow', 'left_hip', 'left_knee',
                             'right_hip', 'right_knee']

JOINTS_WITHOUT_ORIENTATION = ['left_hand', 'right_hand', 'left_foot', 'right_foot']

JOINTS = JOINTS_WITH_ORIENTATION + JOINTS_WITHOUT_ORIENTATION


#+end_src

**** Pose data structures
#+begin_src python
import numpy

class Joint:
  position = None;
  orientation = None;
    
  def __str__(self):
    return "Joint[\n Position: %s,\n Orientation:\n %s ]" % (self.position, self.orientation)
      

def parse_joint(data):
  joint = Joint();
  if len(data) > 4:
    joint.position = numpy.array(data[10:13]) / 1000;
    joint.orientation = numpy.array(data[0:9]).reshape((3,3));
  else:
    joint.position = numpy.array(data[0:3]) / 1000;
  return joint
  

class Pose:
  joints = dict();
   
  def __init__(self, data):
    pos = 1;

    for joint_name in JOINTS_WITH_ORIENTATION:
      joint = parse_joint(data[pos:pos+14]);
      pos += 14;
      self.joints[joint_name] = joint;

    for joint_name in JOINTS_WITHOUT_ORIENTATION:
      joint = parse_joint(data[pos:pos+4]);
      pos += 4;
      self.joints[joint_name]  = joint;
#+end_src

**** RVIZ visualization
***** Node setup
#+begin_src python
import roslib;
import rospy;
import math;
from visualization_msgs.msg import Marker
from visualization_msgs.msg import MarkerArray

topic = 'visualization_marker_array'
publisher = rospy.Publisher(topic, MarkerArray)

rospy.init_node('skeleton_pose_visualizer')

#+end_src

#+RESULTS:

***** ROS messages
#+begin_src python
def create_joint_message(joint, id=0):  
  marker = Marker()
  marker.header.frame_id = "/skeleton"
  marker.type = marker.SPHERE
  marker.id = id
  marker.action = marker.ADD
  marker.pose.position.x = joint.position[0]
  marker.pose.position.y = joint.position[1]
  marker.pose.position.z = joint.position[2]
  marker.scale.x = 0.05
  marker.scale.y = 0.05
  marker.scale.z = 0.05
  marker.color.a = 1.0
  marker.color.r = 1.0
  marker.color.g = 1.0
  marker.color.b = 0.0

  return marker

  
from geometry_msgs.msg import Point

def create_link_message(pose, id=0):

  def pos2Point(joint):
    return Point(joint.position[0], joint.position[1], joint.position[2]);

  points = []
  for jointName1 in LINKS.keys():
    for jointName2 in LINKS[jointName1]:
      joint1 = pose.joints[jointName1];
      joint2 = pose.joints[jointName2];
      points.append(pos2Point(joint1));
      points.append(pos2Point(joint2));

  marker = Marker()
  marker.header.frame_id = "/skeleton"
  marker.type = marker.LINE_LIST
  marker.id = id
  marker.action = marker.ADD
  marker.scale.x = 0.02
  marker.color.a = 1.0
  marker.color.r = 1.0
  marker.points = points

  return marker


  
def create_pose_message(pose):
  markerArray = MarkerArray()
  id = 0
  for joint in pose.joints.values():
    markerArray.markers.append(create_joint_message(joint, id))
    id += 1    
    markerArray.markers.append(create_link_message(pose, id))

  return markerArray

#+end_src

#+begin_src python
def visualize_frame(frame, dataset_person=DatasetPerson()):
  publisher.publish(create_pose_message(dataset_person.get_pose(frame)))


import time

def visualize_interval(start_frame=1, end_frame=1000, dataset_person=DatasetPerson()):
  for frame in range(start_frame, end_frame):
    visualize_frame(frame, dataset_person);
    time.sleep(1.0/25.0)
#+end_src

** gplvm
#+begin_src python
import numpy as np
import string
import matplotlib.pyplot as pb
import GPy

def learn_GPLVM(activity):
  p = DatasetPerson();
  p.load_activity(activity);
  data = p.get_processed_data();
  input_dim = 3
  kern = GPy.kern.rbf(input_dim)
  # kern = GPy.kern.periodic_exponential()
  m = GPy.models.BCGPLVM(data, input_dim=input_dim, kernel=kern)

  # initialize noise as 1% of variance in data
  # m['noise'] = m.likelihood.Y.var()/100.
  m.optimize('scg', messages=1, max_iters=1000)

  return m
#+end_src

#+begin_src python
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

def visualize_latent_model(model):
  fig = plt.figure()
  ax = fig.add_subplot(111, projection='3d')

  xs = model.X[:,0]
  ys = model.X[:,1]
  zs = model.X[:,2]
  ax.scatter(xs, ys, zs)

  ax.set_xlabel('latent 1')
  ax.set_ylabel('latent 2')
  ax.set_zlabel('latent 3')

  plt.show()

#+end_src

#+begin_src python
import GPy
#+end_src

** Sandbox
#+begin_src python

#+end_src

* Unsorted                                                         :noexport:
** Links
- [[http://glowingpython.blogspot.de/2012/10/visualizing-correlation-matrices.html][visualizing a correlation matrix]]
** Cites
*** Simplicity
Simplicity is a great virtue but it
requires hard work to achieve it
and education to appreciate it.
And to make matters worse:
complexity sells better.
Edsger Wybe Dijkstra 

Simplicity is the ultimate
sophistication.
Leonardo da Vinci
** Ideas
* Deprecated                                                       :noexport:
** Lisp
*** Configuration
**** Prerequisites
***** Common lisp
- sbcl
- quicklisp
***** System
- ros (hydro)
- gsl library

**** Start roscore
#+begin_src sh :results output :shebang "#!/bin/bash" :session test
 roscore&
#+end_src


**** Common Lisp Initialization
[[http://common-lisp.net/project/asdf/asdf/Configuring-ASDF.html][Configuring ASDF]]

Install all ros related packages. e.g:
#+begin_src sh
 sudo apt-get install ros-hydro-roslisp*
 sudo apt-get install ros-hydro-cl-*
#+end_src


We want to run common lisp ros code outside of catkin.
Add the following two files:

***** ~/.config/common-lisp/source-registry.conf.d/roslisp.conf
#+begin_src lisp
(:tree "/opt/ros/hydro/share/")
#+end_src

***** ~/.config/common-lisp/source-registry.conf.d/msgs.conf
#+begin_src lisp
(:tree "/opt/ros/hydro/share/common-lisp/ros/")
#+end_src

*** Visualization

**** Lisp
***** Common lisp packages Initialization
#+begin_src lisp :session 
  (ql:quickload "cl-ppcre")
  (ql:quickload "gsll")
  (ql:quickload "roslisp")
  (ql:quickload "alexandria")

#+end_src

#+RESULTS:
| alexandria |


#+begin_src lisp  :session :results silent
  ; making sure that roslisp is loaded
  (asdf:operate 'asdf:load-op :roslisp)

  ; making really sure that roslisp is loaded
  (ros-load:load-system :roslisp)
  (ros-load:load-system :cl-transforms)  
  (ros-load:load-system :visualization_msgs-msg)
#+end_src

***** Utils
****** Data set reading utils
#+begin_src lisp :session
  (defun read-file (path)
    (let ((lines (make-array 1 :fill-pointer 0)))
      (with-open-file (stream path)
        (do ((line (read-line stream nil)
                   (read-line stream nil)))
            ((null line))
          (vector-push-extend line lines)))
      lines))
#+end_src

#+RESULTS:
: READ-FILE


#+begin_src lisp :session
(defun read-frame (frame &optional (data *annotations*))
    (mapcar #'read-from-string  (cl-ppcre:split "," (aref data frame))))
#+end_src

#+RESULTS:
: READ-FRAME

****** List -> multidimensional array (matrix)
#+begin_src lisp :session
(defun list->matrix (lst)
           (let ((array (make-array '(3 3))))
             (setf (aref array 0 0) (first lst))
             (setf (aref array 0 1) (second lst))
             (setf (aref array 0 2) (third lst))
             (setf (aref array 1 0) (fourth lst))
             (setf (aref array 1 1) (fifth lst))
             (setf (aref array 1 2) (sixth lst))
             (setf (aref array 2 0) (seventh lst))
             (setf (aref array 2 1) (eighth lst))
             (setf (aref array 2 2) (ninth lst))
             array))
#+end_src

#+RESULTS:
: LIST->MATRIX

***** Data: Joint/Skeleton objects
 #+begin_src lisp  :session
   (defstruct joint
     position 
     orientation)
   
   (defstruct skeleton
     frame
     joints
     links)
   
   (defmacro x-pos (joint)
     `(first (joint-position ,joint)))
   
   (defmacro y-pos (joint)
     `(second (joint-position ,joint)))
   
   (defmacro z-pos (joint)
     `(third (joint-position ,joint)))
#+end_src

 #+RESULTS:
 : Z-POS

***** Function: Parse the data and create a skeleton object

#+begin_src lisp :session 
  
  (defvar *links*  '((torso neck) (torso left_shoulder) (torso right_shoulder)
                     (torso left_hip) (torso right_hip)  (neck head) 
                     (left_shoulder left_elbow) (right_shoulder right_elbow)
                     (right_elbow right_hand) (left_elbow left_hand)
                     (right_shoulder left_shoulder)
                     (left_hip left_knee) (right_hip right_knee)
                     (left_knee left_foot) (right_knee right_foot)
                     (left_hip right_hip)))
  
  (defvar *joints-with-orientation* '(head neck torso left_shoulder left_elbow 
                          right_shoulder right_elbow left_hip left_knee
                          right_hip right_knee))

  (defvar *joints-without-orientation* '(left_hand right_hand left_foot right_foot))

  (defvar *joints* (append *joints-with-orientation* *joints-without-orientation*))

#+end_src

#+RESULTS:
: *JOINTS*


#+begin_src lisp :session 
  (defun create-joint-from-list (lst)
    (make-joint
     :orientation (list->matrix (subseq lst 0 9))
     :position (subseq lst 10 14)))
  
  (defun create-skeleton-from-data (lst)
    (let ((start 0))
      (flet ((next-chunk (size)
               (let ((result (subseq lst start (+ start size))))
                 (setf start (+ start size ))
                 result)))
        (let ((frame (next-chunk 1))
              (joints nil)
              (links *links*))
          (dolist (joint-name *joints-with-orientation*)
            (push (cons joint-name (create-joint-from-list (next-chunk 14))) joints))
          
          (dolist (joint-name *joints-without-orientation*)
            (push (cons joint-name (make-joint :position (next-chunk 4))) joints))
          
          (make-skeleton :frame frame :joints joints :links links)))))  
#+end_src

#+RESULTS:
: CREATE-SKELETON-FROM-DATA

***** Function: create ros messages

#+begin_src lisp  :session
  (defun create-joint-message (joint id)
    (let ((pos (joint-position joint)))
      (roslisp:make-message 
       "visualization_msgs/Marker"
       (stamp header) (roslisp:ros-time)
       (frame_id header) "/skeleton" 
       (id) id
       (type)  (roslisp-msg-protocol:symbol-code
                'visualization_msgs-msg:<marker>
                :sphere)
       (action) (roslisp-msg-protocol:symbol-code
                 'visualization_msgs-msg:<marker>
                 :add)
       (x position pose) (/ (first pos) 1000)
       (y position pose) (/ (second pos) 1000)
       (z position pose) (/ (third pos) 1000)
       (x scale) 0.03
       (y scale) 0.03
       (z scale) 0.03
       (g color) 1.0
       (a color) 1.0
       (lifetime) 100)))
#+end_src

#+RESULTS:
: CREATE-JOINT-MESSAGE

#+begin_src lisp :session
  (defun create-link-list-message (points id)
    (roslisp:make-msg 
     "visualization_msgs/Marker"
     (stamp header) (roslisp:ros-time)
     (frame_id header) "/skeleton" (id) id
     (type)
     (roslisp-msg-protocol:symbol-code
      'visualization_msgs-msg:<marker>
      :line_list)
     (action)
     (roslisp-msg-protocol:symbol-code
      'visualization_msgs-msg:<marker>
      :add)
     (x scale) 0.01
     (r color) 1.0
     (a color) 1.0
     (lifetime) 100
     (points) points))
  
  (defun links->line-points (links joints)
    (let ((points nil))
      (mapcar 
       (lambda (el)
         (let ((p1 (joint-position (cdr (assoc (first el) joints))))
               (p2 (joint-position (cdr (assoc (second el) joints)))))
           (push (roslisp:make-msg "geometry_msgs/Point" 
                                   :x (/ (first p1) 1000)
                                   :y (/ (second p1) 1000)
                                   :z (/ (third p1) 1000)) points)
           (push (roslisp:make-msg "geometry_msgs/Point"
                                   :x (/ (first p2) 1000)
                                   :y (/ (second p2) 1000)
                                   :z (/ (third p2) 1000)) points))) 
       links)
      (map 'vector #'identity points)))
  
#+end_src

#+RESULTS:
: LINKS->LINE-POINTS

#+begin_src lisp :session
      (defun create-skeleton-message (skeleton)
        (let ((index 0) (markers 'nil))
          (mapcar (lambda (el) 
                    (push (create-joint-message (cdr el) index) markers)
                    (incf index))
                  (skeleton-joints skeleton))
          
          (push (create-link-list-message 
                 (links->line-points 
                  (skeleton-links skeleton) 
                  (skeleton-joints skeleton))
                 index) 
                markers)
          (roslisp:make-msg "visualization_msgs/MarkerArray" :markers
                            (map 'vector #'identity markers))))
#+end_src

#+RESULTS:
: CREATE-SKELETON-MESSAGE

***** Visualize a frame

#+begin_src lisp :session
  (defun visualize-frame (frame &optional (data *annotations*) (pub *pub*))
    (roslisp:publish pub 
                     (create-skeleton-message (create-skeleton-from-data (read-frame frame data)))))
#+end_src

#+RESULTS:
: VISUALIZE-FRAME

#+begin_src lisp :session
    (defun visualize-interval (start-frame end-frame &optional (data *annotations*) (pub *pub*) (sleep-time 0.05))
      (loop for frame from start-frame to end-frame do
        (progn
          (visualize-frame frame data pub)
          (sleep sleep-time))))
#+end_src

#+RESULTS:
: VISUALIZE-INTERVAL

**** Lisp: visualization test

#+begin_src lisp :session
  (ROSLISP:START-ROS-NODE "test")
  (defvar *pub* (ROSLISP:ADVERTISE "visualization_marker_array" "visualization_msgs/MarkerArray"))
  (defvar *annotations* (read-file "/work/Data/human_activities/data1/0512164529.txt"))

  (visualize-interval 1 1000)
#+end_src

#+RESULTS:
: NIL

* Footnotes

[fn:1] Human Activity Detection from RGBD Images, Jaeyong Sung, Colin Ponce, Bart Selman, Ashutosh Saxena. In AAAI workshop on Pattern, Activity and Intent Recognition (PAIR), 2011. 
[fn:2] RGB-D Camera-based Daily Living Activity Recognition - Chenyang Zhang, Student Member, IEEE and Yingli Tian, Senior Member, IEEE
 
