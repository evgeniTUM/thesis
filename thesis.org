#+PROPERTY: header-args:lisp :results replace :session
#+PROPERTY: header-args:python :results none :session test :exports none

#+COLUMNS: %25ITEM %TAGS %PRIORITY %TODO

* LaTeX                                                            :noheading:

#+BEGIN_SRC emacs-lisp :exports none
(setenv "PYTHONPATH" (concat (getenv "PYTHONPATH") ":./code/spencer"))
#+END_SRC

#+TITLE: Online activity recognition through kernel methods
#+AUTHOR: Evgeni Pavlidis

#+LaTeX_CLASS: scrbook
#+LaTeX_CLASS_OPTIONS: [11pt,a4paper,bibtotoc,idxtotoc,headsepline,footsepline,footexclude,BCOR12mm,DIV13,openany]
#+LaTeX_CMD: xelatex

# --- Packages
#
#+LaTeX_HEADER: \usepackage{pdfsync}
#+LaTeX_HEADER: \usepackage{scrpage2}

#+LaTeX_HEADER: \usepackage{hyperref}

#+LaTeX_HEADER: \usepackage{palatino}
#+LaTeX_HEADER: \usepackage{pifont}
#+LaTeX_HEADER: \usepackage{rotating}
#+LaTeX_HEADER: \usepackage{float}
#+LaTeX_HEADER: \usepackage[utf8]{inputenc}
#+LaTeX_HEADER: \usepackage{marvosym}

#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amsfonts}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{bm}
#+LaTeX_HEADER: \usepackage{textcomp}

#+LaTeX_HEADER: \usepackage{makeidx}
#+LaTeX_HEADER: \usepackage{subfigure}
#+LaTex_HEADER: \usepackage{graphicx}

#+LaTeX_HEADER: \usepackage{todonotes}
#+LaTeX_HEADER: \usepackage{setspace}


# --- Options
#
#+LaTeX_HEADER: \pagestyle{scrheadings}



# --- TITLE ---
#
#+LaTeX_HEADER: \let\OldMaketitle\maketitle
#+LaTeX_HEADER: \renewcommand{\maketitle}{
#+LaTeX_HEADER: \pagenumbering{roman} 
#+LaTeX_HEADER:
#+LaTeX_HEADER: \OldMaketitle
#+LaTeX_HEADER: }





# --- Table of Contents
# 
#+OPTIONS: toc:nil   
#+TOC: headlines 2

# --- Bibliography
#
#+BIBLIOGRAPHY: bibliography plain limit:t
#+STYLE: &lt;link rel="stylesheet" type="text/css" href="css/org.c


#+begin_latex
\newcommand{\TODO}[1]{\todo[color=red]{#1}}
\listoftodos 
\pagenumbering{arabic} 
#+end_latex


\TODO{cite:software packages and tools used}
\TODO[color=red,textsize=large]{cite:datasets (mocap, daily activities, ms activities)}
\TODO{Check bibliography style and data!!!}


* Introduction

** Motivation
*** Online (active) learning of human activities
Use gaussian processes to learn new activities in real time
*** Evaluate Gaussian processes against different ml algorithms for activity recognition
Evaluate the performance of GPs in relation to the other solutions

** Scope
*** The SPENCER project
\missingfigure{spencer prototype robot (Bender B21)}

\missingfigure{spencer robot}
With the modern technologies (ms Kinect SDK, primesense ...) it is possible to decouple skeleton tracking and learning

\missingfigure{skeleton representation}

** Notations and conventions
- Matrices uppercase bold
- Vectors lowercase bold
- Constants lowercase
- Parameters lowercase greek letters
- GP notation
- Expectation notation <...>
** Structure of the thesis
- Introduction ::
   The first chapter introduced the topic of this work. The motivation and the scope is explained.
- Basic Concepts ::
   This chapters summarizes some basic concepts and models used in our approach. For the most part Gaussian Process Regression and Gaussian Process - Latent Variable Models are being explained. It also gives an overview of methods used in similar approaches in the related works section, such as \todo{overview aproaches related work }
- Related Work

- Analysis
  The fourth chapter discusses the advantages and disadvantages of some existing approaches towards online activity recognition.
- Approach
  In the fifth chapter the two approaches are being discussed. The first one is an implementation of "Discriminative Sequence Back-constrained {GP}-{LVM} for {MOCAP} based Action Recognition}" ebib:_discriminative_2013. The second one being a novel approach which is learns a motion flow field in the latent space through a Gaussian Process Regression.
- Results and Outlook
  The last chapter summarizes the results of the two approaches and gives a brief outlook of future improvements.
* Basic Concepts
This chapter introduces some basic concepts needed to understand the rest of this work. First an high-level overview is given on machine learning and its terminology. After that the gaussian distribution is presented in its univariate multivariate variants. Following is an explanation of Gaussian Processes, their different interpretations and properties. Last the Gaussian Process - Latent Variable Model is being introduced along with some extensions for learning a backward mapping and optimizing it for discrimination in the case of multiple-classes. 
 
** Machine Learning
*** Supervised learning
Supervised learning is the task of classification or regression when the data is being labeled. 
The algorithm then takes the labeled samples (and maybe some confidence values) and infers the model parameters (or hyperparameters) accordingly. 

There are two distinct cases:

**** Classification
\missingfigure{classification example}
Classification the task of learning which category a sample belongs to. A prominent example is Spam filtering. By taking a large number of emails which are labeled either as spam or as ham (regular email), the algorithm deduces a model which can classify unknown samples into these two categories.

**** Regression
\missingfigure{regression example}
Regression is a terminus in machine learning and means function approximation. Here the domain of the samples label is continuous. 

In most cases we search for a good model that explains the data we have. Parametric models, for example, try to learn the ...
When searching for an appropriate model it is also important that we try to capture the underlying relationship without compromising the generalization property, which is the ability of the model to correctly predict unseen samples. The case that an algorithm learns the relationship of the data that is used to train the model (training data) but cannot predict new samples is called overfitting. The opposite is called underfitting. 


Very often the parameter search is done by maximizing the probability of the data given the model parameters. 

$$ \operatorname{arg\,max}_{\bm{\theta}} p(\bm{X} | \bm{\theta}) = \operatorname{arg\,max}_{\bm{\theta}} \frac{p(\bm{\theta}|\bm{X}) * p(\bm{X})}{p(\bm{\theta})} $$

where $\theta$ are the model parameters and $X$ is the data.

*** Unsupervised learning
In contrast to supervised learning in unsupervised learning we have no labeled data i.e. there is no supervisor giving each sample a category (classification) or a value (regression). In this case we can only derive properties of the generation process. Therefore we try to detect patterns in the unlabeled data. These pattern may be clusters of similar samples or a lower dimensional generative manifold from which the samples are generated. The last one is called Dimensionality Reduction which will be also a subject in this work. ebib:bishop_pattern_2006 

\missingfigure{dimensionality reduction example}

*** Generative models
Generative methods model the underlying process which generates the data. In Bayesian terms we model the likelihood and the. Thus more data is needed to find an appropriate model. On the other side the model is very flexible and many attributes have a natural interpretation. An example of this is \todo{generative model example}

*** Discriminative models
A discriminative model is only concerned with modeling the actual posterior. This way fewer samples are needed to find the model parameters but by not taking the prior into account the model becomes more generative and is susceptible to overfitting.

*** Online learning
Algorithms which can be gradually optimized towards a good solution using streaming batches of samples are considered to do online learning. 

*** Active learning
Very often the bottleneck of powerful supervised learning techniques is that they rely on correctly labeled data. Since labeling has to be performed by a human it is very difficult and costly to label large amount of data. By identifying more important samples by their entropy, thus information ability of selecting a good model, it is possible to achieve good results with fewer samples.

Letting the algorithm select such samples and query only their labels from a human, who is now actively participating in the learning loop, is called active learning. 

** The gaussian distribution
*** Univariate gaussian distribution
In the one dimensional case the gaussian distribution is well known and understood. Moreover many processes in nature can be modeled with this distribution and for this reason it is also called the Normal distribution. The probability of an event is very high on a certain "point" (its meain value $\mu$) and it drops quickly on each side with the standard deviation $\sigma$.

$$ \mathcal{N}(\mu, \sigma^2) = \frac{1}{\sigma  \sqrt{2 \pi}}e^{-\frac{x-\mu}{2 \sigma^2}} $$

One disadvantage of this distribution which we can see from the above formula is that it can model only one hypothesis. This is also the case for the gaussian distributions of multiple (multivariat gaussian distribution) and infinite (gaussian process) dimensions.

*** Multivariate gaussian distribution
The multivariat gaussian distribution is the generalization of the gaussian distribution in higher dimensions.

$$ \mathcal{N}(\bm{\mu}, \bm{\Sigma}) =  \frac{1}{  \sqrt{(2 \pi)^d} |\bm{\Sigma}|}
e^{-\frac{1}{2} (\bm{x} - \bm{\mu})^T \bm{\Sigma}^{-1}  (\bm{x} - \bm{\mu})} $$

The two parameters of the distribution are:
- mean :: $\bm{\mu} = E[x]$ Representing the most probable vector
- covariance :: $\bm{\Sigma}$ Representing the covariance for each pair of the elements of the random vector: $\bm{\Sigma}_{ij} = Cov[x_i, x_j]$

The exponent is mahalanobis distance, which measures the distance of a point to the ellipsoid defined by the covariance matrix.\todo{cite}


** Gaussian Processes
Consider the multivariate gaussian distribution above. If we want to model the distribution of discrete function defined over a finite interval,
we can treat each element of the vector $\bm{x}$ as an point of the function. Thus we can we can view the multivariate gaussian distribution as a probability function over the function space. Letting the dimensionality $d$ go to infinity (the distance between each point goes to zero) we can model continuous functions. ... ueberleitung GPs ... 

In this case the mean is a point in the function space, thus a function $E[\bm{x}] = f(x)$. And because of the fact that we now have infinite dimensions the covariance can be seen as an "/infinite/ matrix/", thus a function of two elements: $Cov(x,y)$.
 

It can be seen as a gaussian distribution in function space. ebib:rasmussen_gaussian_2006

A gaussian process can be also seen as the bayesean posterior consisting of
the product of the (gaussian) functional prior and the observed
samples.??? Another view is a kernelized regression with infinite
parameters. ebib:rasmussen_gaussian_2006

A gaussian process is a non-parametric model and is governed by the hyperparameters of the used kernel. This also means that the model is less prune to overfitting. 


*** Regression
\missingfigure{nice GP regression example}

*** Learning
In the case of a GP the learning phase is different than in parametric models, where the model parameters are inferred from the data.  
GPs hyperparameter learning by variational optimization (data fit term + cov. regularizer)

In contrast to parametric models gaussian processes are less prune to overfitting because of the covariance regularizer term.


$$ E(\theta) = \frac{1}{2}\log({K}) - \frac{y^T K^{-1} y}{2} $$

*** Classification
Classifying with GPs is a little more involved, because of the
discriminative function and the fact that the likelihood \todo{explain
problems of GP classification right} is not a Gaussian. For this
reason different models exist which try to approximate this
likelihood.

*** Advantages
**** non parametric
Because the model is not parametric it does not suffer from  

**** probabilistic
The hyperparameters can be interpreted. The lenghtscale controls how much neighboring points contribute to the covariance of the function.

**** generative


**** nice for Bayesian
**** linear algebra operations (marginals and conditionals)
*** Disadvantages
**** Unimodal
**** susceptible to outliers
The student-t distribution is robust against outliers but is much harder to deal with.
**** high computational complexity
$\mathcal{O}(n^3)$

There are sparse GP methods which approximate the model. An example are the informative vector machines (IVM) which reduce the complexity to $\mathcal{O}(d^2 n)$ where d is a subset of the samples selected by maximum entropy. ebib:lawrence_fast_2003

**** memory heavy
A GP saves all data points...


*** Algorithms
**** Sparse GPs (IVM)
***** IVM for multiple classes ebib:seeger_sparse_2004

** GP-LVM

The GP-LVM performs a non-linear dimensionality reduction from an observed space$X$ to a latent space $Y$ ebib:lawrence_probabilistic_2005
It does this by maximizing the likelihood $$p(Y|X) = p(Y|f)p(f|X)$$ using a gaussian prior for the mapping $f$. Technically it a GP-LVM is a product of Gaussian Processes which model a regression of the mapping from observed space to one latent dimension. \todo{formulas etc.} 
The model learns a (non-linear) mapping from latent space to observed space. This means also that if we want to compute the latent position of a new observed sample we have to compute the ...\todo{elaborate GP-LVM}. Using a linear kernel the model generalizes to \todo{PCA} PCA. By using a non linear kernel a non-linear mapping is inferred making it a very strong latent variable model.

\missingfigure{example GP-LVM, skeleton}

*** PCA
Tipping and Bishop, Journal of the Royal Statistical Society (1999)


*** Back-constraints GP-LVM
One problem with this model is that it does not preserve local distances in the latent space.This is because it tries to explain the data by moving distant samples from the observed space also far apart in the latent space. This problem is addressed by Lawrence et al. in the back-constrained GP-LVM ebib:lawrence_local_2006. A mapping $g_i(y_i) = x_i$ is introduced which constrains the points in latent space to be more near if they are also near in the observed space. Instead of optimizing directly on $X$ the back-constrained GP-LVM optimizes using the  mapping instead. 

Having this back-constraints also gives us a mapping from observed space to latent space which can be used to project a new sample into the latent space without costly maximum likelihood estimates. 
\missingfigure{example BCGPLVM}
 
*** Bayesian GP-LVM
An interesting approach for computing the likelihood of the latent variable mapping was proposed in ebib:titsias_bayesian_2010. By using a variational method it becomes possible to marginalize over $X $. Doing so the mapping can be learned together with an \todo{explain ARD} ARD kernel. This way the dimensionality of the manifold can be learned from the data. 

*** Discriminative GP-LVM
Another improvement in the context of classification in latent space is the Discriminative GP-LVM ebib:urtasun_discriminative_2007. Using the GDA \todo{elaborate GDA} a prior is being enforced on the LVM which ensures that samples from one class are more clustered and different classes are more separated in the latent space. This is done by maximizing the between-class separability and minimizing the within-class variability while optimizing the log likelihood of the GP-LVM.ebib:urtasun_discriminative_2007


*** Subspace GP-LVM

*** Manifold Relevance Determination
Combining the Subspace GP-LVM with the variational approach and the ARD kernel it is possible to learn the manifold \todo{explain MRD}.ebib:damianou_manifold_2012

*** GP-LVM for human motion
As the space of human motion is high-dimensional (spatio-temporal) dimensionality reduction is crucial for a number of models dealing with human motion (e.g. ebib:fan_gaussian_2011l).
The GP-LVM preserve the distances in the mapping and are therefore suitable to model human motion with high noise of the poses see Urtasun DGPLVM
Newest addition is ebib:jiang_modeling_2014

*** Advantages

*** Disadvantages

**** The objective function is non-convex
This in fact is the biggest problem as it limits its use on real world data, because for more complex data and (latent manifold structures) there will likely be many local minima. For this reason it is crucial to choose a good initialization. Examples are PCA, Local Linear Embedding or Isomap.

* Related work
** Overview
*** a survey on vision based action recognition ebib:poppe_survey_2010
*** machine vision for human activities: a survey ebib:turaga_machine_2008
** Histogram based approaches
*** Motion history image 

*** Motion energy image

** Sung et al. ebib:sung_unstructured_2012
*** Features: Skeleton data + HOG features of RGBD image and depth image 
*** Naive classification: SVM
*** Maximum entropy markov model
Solved via max-flow/min-cut
** RGB-D Camera-based Daily Living Activity Recognition ebib:zhang_rgb-d_2012
*** Bag of Features
#+begin_src dot :file figures/bag-of-features-approach.png
   digraph pipeline {
     label="pipeline";
     rankdir=LR;

     node [color=blue, shape=box];
  
     feature_extraction;
     k_means;
     vector_quantization;
     centroids[shape=ellipse];

     feature_extraction -> k_means -> vector_quantization -> centroids;

     subgraph {
        label =  "bag_of_features";
     }     
  }
#+end_src

#+RESULTS:
[[file:figures/bag-of-features-approach.png]]

See [fn:2]

*** Features: Structural and Spatial motion
Feature capturing transition between two frames
*** Bag of Features approach (historgram of features)
*** Other: People identification (reidentification)
** Learning Human Activities and Object Affordances from RGB-D Videos 
*** Learning both: activities and object detection/affordance
*** Using Markov Random Field and SVM for learing
** Eigenjoints ebib:yang_effective_2013
** Gaussian Process - Latent Conditional Random Field (GP-L CFR)
ebib:jiang_modeling_2014 use GP-LVM to reduce dimensionality of human motion. (earlier approach was Gibbs sampling) 
** GPDM
In ebib:wang_gaussian_2005 the dynamics of the latent space is being modeled from time series data. In ebib:wang_gaussian_2008 this model is being used to model human motion by applying a GP-LVM to the high-dimensional mocap data and simultaneously learning the dynamic transition in the latent space:

                     $$ x_{t_{k+1}} = f(x_{k}) $$

$f(x)$ is being modeled by a gaussian process.

This model was applied for activity recognition in ebib:jamalifar_3d_2012 where the classification is done through an SVM in the hyperparameter space. (only 2? features)
** Joint Gait Pose Manifold
The JGPM models the activity and the gait in an common latent space. This way several samples from different persons are modeled with the addition with the gait and do not corrupt the class model.
** Dynamic time warping

** See also 
A class of space-varying parametric motion fields for human activity recognition


* Analysis
** Observations
- Person identification through covariance matrix of movement see. ebib:kumar_human_2012
- Difference between activity and action
  Activities are composed of actions
- Context information can tremendously help in classification of activities (e.g. object detection and human anticipation)
- Skeleton data is sufficient for classification (ebib:ibbt_does_????)
  and also robust to changes in appearance (most state-of-the-art methods work with visual features)
  and also unobtrusive and sensible data doesn't need to be stored (like face features etc.)
- Knowing which activity a human performs helps tremendously in classifying age and gender! (in the case that we do both)
** Approaches
*** make the features invariant ebib:theodorakopoulos_pose-based_2014 
- view invariant (pos rel to torso)
- scale invariant (normalize length...)
   ... time ?? invariant
*** Discriminative Sequence BCGPLVM
Use this to find the activity
**** DTW between walking and walking backwards very big ...
**** not taking temporal dimension into account
*** Use the Joint Gate and Pose Manifold for age and gender detection
*** GPDM
**** approach to classify by hyperparameters not optimal
*** VarGPDS
**** very slow computation
*** Classify by dynamics of the skeleton (this should bring good classification)
**** GPDM can model the dynamics of the movement
**** has good properties (gaussian processes)
**** has intrinsic dim reduction
**** ?? shared GP-LVM to model different activities in the same latent manifold ??
** Problems and solutions
*** limited sample data - probabilistic model + discriminative
Probabilistic (and generative ??) models are more accurate using fewer samples, because they model the probability directly ...  
*** high dimensional - dim reduction(gp-lvm)

*** classification - BC GP-LVM + discriminative
*** time series data - GPDM
An can be modeled as a sequence of consecutive poses. Hence a dynamical model. By using a dynamical model classification becomes more discriminative. 
*** confidence is important !!!
Using a probabilistic model (especially gaussian processes) we also get a confidence which in turn can be used for active learning
*** high dim. noise => GP-LVM is very robust because of the nature of optimization (distance is preserved instead of locality)
** Assumtpions
*** Skeleton tracking is correct and stable
For the algorithm we assume that the skeleton extraction from RGBD data works as expected.
This is far from the truth with current skeleton tracking algorithms but we also get confidences of the poses.
This way we can prune a large number of incorrect poses and because we model the dynamics and do not compare poses this is not a big problem.
*** Smooth skeleton transition !!!
*** Correctly labeled samples (no outliers)
** Ideas
*** Presentation
**** make clear what is your contribution !!!
**** Black slides (important points)
*** Model
**** Take best three activites (uncertainty) with threshold
**** SPENCER: can help for (head tracking (bounding box), and pose estimation)
**** Use hand and/or head features
***** Head direction is important
***** Hand structure is very important for most tasks
***** Object interrelation ???
***** Use HOG for hand features only
**** Bhattacharyya distance

**** bag of features 
- no time dependency
- no online capable because of k-means clustering
**** maximum entropy markov model
- complex, performance not that good
**** GP-LVM
- good to reduce the dimensionality
- used in some papers
**** Learn a model of an activity and compare it with the help of a covariance function
*** Analogy LVM <-> marionettes
* Approach
** Datasets
*** [[http://pr.cs.cornell.edu/humanactivities/data.php][Cornell Activity Dataset]]

Active learning using Gaussian Processes.
We will use the "Cornell Activity Datasets (CAD-60 & CAD-120)"[fn:1] to learn and evaluate 
the performance of an implementation of Gaussian Processes. 

The data set s consist of an sequence of frames which include: 
- Image data
- RGBD data
- Skeleton information: (joint position and orientation)
- annotated meta information (e.g. activity)
** Dynamic time warping with mahalanobis distance 
:( exists already 
Learning Mahalanobis distance for DTW based online signature verification 
maybe use fisher discriminant analysis as in the disr. seqBCGPLVM

** TODO Discriminative Sequence Back-Constrained GP-LVM
In the paper "Discriminative Sequence Back-Constrained GP-LVM for MOCAP Based
Action Recognition"ebib:_discriminative_2013 the authors propose a method for
classifying MOCAP actions.

#+begin_src dot :file figures/seq-gplvm-approach.png
   digraph pipeline {
     label="Pipeline: Sequence back-constrained GP-LVM pipeline ... CITATION";

     node [color=blue, shape=box];

     subgraph clusterLearning {
        style = filled;
        label =  "learning";
        feature_extraction -> gplvm -> latent_space -> centroids;
        sequence_constraints -> gplvm;
        discriminative_constraints -> gplvm;

        discriminative_constraints [shape=ellipse, label="discriminative  constraints"];
        sequence_constraints [shape=ellipse, label="sequence constraints"];
        { rank=same; gplvm; sequence_constraints; discriminative_constraints; }
     }

     centroids -> SVM;

     subgraph clusterRecognition {
              label = "recognition";
              sequence_mapping -> SVM -> activity_class;           
     }
  }
#+end_src

#+RESULTS:
[[file:figures/seq-gplvm-approach.png]]


By using a similarity feature for the sequences in
the observed space and constraining the optimization to preserve this measure
the local distances between the sequences are transferred into the latent space.
This has two advantages. First of all the sequences have a meaningful clustering
in the latent space. Second by also learning the back-constraint it is possible
to calculate the centroid of a sequence in the latent space directly without
maximizing a likelihood. This in turn is being used to do real-time
classification for actions. The mapping is defined as a linear combination of
the DTW distance between every other sequence. For every latent dimension $q$ we
have:

              $$ g_{q}(Y_s) = \sum_{m=1}^{S} a_{mq} k(Y_s,Y_m) $$

where the similarity measure is $k(Y_s, Y_m) = \gamma e^{DTW(Y_s, Y_m)}$. This
measure is to be preserved in the latent spaces.

       $$ g_q(Y_s) = \mu_{sq} = \frac{1}{L_s} \sum_{n \in J_s} x_{nq} $$

This constraints are being enforced in the optimization by adding Lagrangians to the objective function.

\missingfigure{example of discriminative and back-constrained latent space}

Furthermore, by applying the Discriminative GP-LVM we ensure that poses of different activities are separated from each other and poses from similar activities are located closer together. This ensures that the centroid of an activity is more informative and thus discriminative. The Discriminative GP-LVM works by also maximizing the between class variance and minimizing the in-class similarity ebib:urtasun_discriminative_2007 \todo{expain D GP-LVM properly}
Also by applying the Discriminative GP-LVM the clustering of similar actions and
the distances of different actions is enhanced which allows for a better
classification. Recognition is being done by applying the mapping above to the
new sequence and using a SVM in the latent space.

*** Advantages
Recognition can be done in real time by using the learned back constrained. The centroid in the latent space is being calculated for the whole sequence and classified by the SVM. 

*** Improvements
The GP-LVM learns a mapping for each pose but does not consider velocities and accelerations. If we take a pose along with its first and second moments (let us call them poselets) as the high-dimensional space we allow for the temporal displacements to be also modeled.
The latent space represents the poselet and the DTW kernel in the constraint captures also the motion of the activity.

*** Shortcomings
As the optimization for GP-LVM is determined by the above similarity measure and the discriminative criterion online optimization is very difficult. It is thus highly likely that performing a gradient online optimization will be stuck in an local minimum.

Also one problem with the real-time recognition is that determining when a activity has ended/begun is very difficult. Also as we do not know how long a sequence is we have to calculate the centroid for several time frames.

** Extensions:
*** Learn poselets (pose and velocities) to capture dynamics
*** Use mahalanobis for the DTW 
** GP- Latent Motion Flow Field (based on the gp regreesion flow)
Many models which use GP-LVM to reduce the high dimensional space into fewer dimension. These approaches make the problem more feasible but the problem remains how to do classification for time-series data. Human motions are mostly characterized by the dynamics of the model (temporal dimension). So we have to compare trajectories in the latent space. One idea is to use GPRF as classification can be done using second order dynamics which should give better results. Going further the activity itself is characterized by the first and second moments of the trajectory function. By explicitly modeling the velocity of the trajectory we can take changes in the joint movement into account.

The Gaussian Process Regression Flow ebib:kim_gaussian_2011 can be used to model the trajectories in the latent space.




The GP-LMF method is inspired by this model. The difference being that in the case of activity recognition we do not know the starting position and also the trajectories can have significantly different lengths. For this reason it is very difficult to normalize with respect to the time dimension.
Nevertheless, resulting from the properties of Gaussian Process regression, we have also a dense mean flow field and dense variances. This allows us perform efficient and robust online recognition in the latent space.

This model is attractive for two reasons. First real-time classification of incomplete trajectories is possible. Second it is possible to do online learning by simply adding the new class as a new flow field to the pool of GPs. It is very difficult to adjust the other models for online learning, because of the problem that we can get stuck in a local minimum when optimizing the parameters of the GP.

The idea is to learn a motion field in the latent space for each activity. This can be achieved by learning the velocity function of the latent point just like in the GPRF model presented above. With the difference that we do not use the spatio-temporal domain but learn only the flow in the latent space. The reason being that we do not have starting and ending positions for each activity and also the lengths can be variable. On top of that we also want to recognize an activity which is being interrupted by another activity, so we can't fix the lengths of the trajectories. 

\missingfigure{example of several flow fields inside latent space}

Each activity has its own flow field. Recognition and prediction is done by calculating the energy of the currently moving point with each different field. The field with the minimum energy represents the most probable activity as the point follows more closely its "current" of motion.

Variances in the speed of performing an activity can be modeled by giving the point in the latent space a mass which can be adjusted in real time.
When a point has greater mass then it needs more energy to be propagated along the flow field (the overall activity is slower) and vice versa.

An advantage of this method is that activities with repetitive motions, such as walking or running, can be learned without using periodic kernels or other means to model them explicitly. Repetitive motions can be seen as just multiple samples of the same motion which define the flow field.

#+begin_src dot :file figures/gplmf-approach.png
digraph pipeline {
        label="Pipeline: Gaussian Process - Latent Motion Flow";

        node [color=blue, shape=box];

        subgraph clusterLearning {
                label = "learning"
        
                subgraph clusterDimReduction {
                        style = filled;
                        label =  "dim. reduction";
                        feature_extraction -> gplvm -> latent_space;
                        back_constraints -> gplvm;

                        back_constraints [shape=ellipse, label="back constraints"];
                        { rank=same; gplvm; back_constraints; }
                }

                latent_space -> numerical_derivative -> GPs -> flow_model;
                
                
        }

        energy_computation -> flow_model [arrowhead=dot, style=dashed];

        subgraph clusterRecognition {
                label = "recognition";
                online_sequence -> energy_computation -> class;           
        }
}
#+end_src

#+RESULTS:
[[file:figures/gplmf-approach.png]]
*** Interpretation
The proposed model has a natural interpretation. A point represents a pose in latent space and an activity is a trajectory in time inside the same space. With the flow field we learn the motion tendencies for each pose. When performing recognition we for each separate flow the energy needed to traverse the flow. If we consider that the point has a mass we can model the speed at which activities are being done. This way we can recognize when a point leaves an activity, which represents a /motion current/, and passes over to some other activity.

*** Advantages
**** Recognition
The current activity is being mapped into the latent space. Through the learned back-constrained. The recognition is being performed solely in the latent space. By propagating the current position by each flow field we can calculate the next possible pose. By comparing the similarity considering the variances we have a measure of how well the current activity resamples each flow field e.g. learned activity.

**** Prediction
If we have detected the activity predicting is simply a matter of propagating the pose through the flow field by taking the mean of the GP.

**** Online learning

**** Natural interpretation
**** Novelty detection (anomaly detection)
In ebib:kim_gaussian_2011 the authors present the ability of the GPRF model for anomaly detection. 
This approach is also suitable for finding new classes as the above energy value can be used to recognize novel activities. The reasoning is that if we cannot find an flow field with a small energy the activity has to be unobserved.

**** Active learning
**** Multiple Hypothesis Prediction
Since we have a GP representing our flow field we can predict future point positions with the mean value. Moreover also having informative variances we can sample several possible trajectories. This can be accomplished using an particle filter. Hence we can have multi-hypothesis predictions along with their probabilities.
*** Problems
**** Dimensionality reduction
Performing a non-linear dimensionality reduction is no easy task. Testing was done with only two dimensions as it easier to visualize the latent space and the resulting flow fields.
A latent space with higher dimension will naturally make the reduction more robust and the field will have a more natural interpretation....

**** Stable class mean flow field
When learning a stable flow field from several samples the field can degenerate with the inclusion of strong variable paths. Therefore it is important to ensure that the algorithm learns stable paths. This can be achieved by sampling uniform random sampling from all samples of the same activity.

\todo{active learning - problem ??}

*** Learning the motion flow field
One problem we encounter by learning the motion flow field from several samples is complexity of the Gaussian Process. There are two solutions for this. The first one is to use a sparse GP model. The second one is to sample points from all samples and use only those that are most suitable for the regression. If we take IVM as the sparse GP model both approaches can be seen as equivalent as the IVM will automatically take the most informative samples.
** Implementation

*** Finding appropriate kernel function



*** Learning the flow field
We deploy GP for learning the flow field which gives us several advantages.

\missingfigure{latent space (several samples of one activity) with flow field}
**** Effects of the hyperparameters

Changing the /lengthscale/ defines how much each point is contributing to the regression process. It can be interpreted as a smoothness factor which governs how strong the interpolation of the flow field is performed on the latent points.

Changing the signal variance controls how much 


\missingfigure{effect of hyperparameters on the resulting flow field}


*** Software
MATLAB - FGPLVM 
Dataset: [[http://mocap.cs.cmu.edu][CMU Motion capture dataset]]
- Emacs/Org-mode
- IPython
- SciPy/NumPy
- GPy
- mlpy





** Bibliography
#+begin_latex   
  \bibliographystyle{plain}
  \bibliography{bibliography}
#+end_latex


* LAB                                                              :noexport:
** Classification
*** Dataset management
#+begin_src python
import glob
import os
import numpy as np


data_set_indices = []
# indices of positions of first 11 joints (joints with orientation)
# 9 ori + 1 conf   +   3 pos + 1 conf = 14 
for joint in range(0,11):
  for x in range(10,13):
    data_set_indices.append(1 + joint*14 + x);

# indices of hands and feet (no orientation)
for joint in range(0,4):
  for x in range(0,3):
    data_set_indices.append(155 + joint*4 + x);
        

default_data_dir=os.getenv("HOME")+'/data/human_activities'

      
class DatasetPerson:

  data_dir = "";
  person = -1;
  direcotory = "";
  activity_label = dict();
  classes = list();
  activity = ''
  data = None

  def __init__(self, data_dir=default_data_dir, person=1):
    self.data_dir = data_dir;
    self.person = person;
    self.directory = data_dir + '/data'+ str(person) + '/';

    # read labels
    with open(self.directory + '/activityLabel.txt') as f:
      self.activity_label = dict([filter(None, x.rstrip().split(',')) for x in f if x != 'END\n']);

    self.classes = list(set(self.activity_label.values()));
    self.activity = self.activity_label.keys()[0]
    self.load_activity(self.activity)


  def load_activity(self, activity):
    self.activity = activity
    file_name = self.directory + activity + '.txt';
    self.data = np.genfromtxt(file_name, delimiter=',', skip_footer=1);

  def get_processed_data(self):
    data = self.data[:, data_set_indices];

    # take relative position of the joints (rel. to torso)
    for row in data:
      torso_position = row[6:9]
      for joint in range(0, 15):
        row[joint*3:joint*3+3] -= torso_position

    return data

  def get_pose(self, frame):
    return Pose(self.data[frame])
#+end_src

*** Visualization
**** Skeleton structure
#+begin_src python
LINKS = {'torso' : ['neck', 'left_shoulder', 'right_shoulder', 'left_hip', 'right_hip'],
         'neck' : ['head'], 
         'left_shoulder' : ['left_elbow'],
         'right_shoulder' : ['right_elbow', 'left_shoulder'],
           'right_elbow' : ['right_hand'], 
           'left_elbow' : ['left_hand'], 
           'left_hip' : ['left_knee', 'right_hip'], 
           'right_hip' : ['right_knee'],
           'left_knee' : ['left_foot'], 
           'right_knee' : ['right_foot'],}



JOINTS_WITH_ORIENTATION = ['head', 'neck', 'torso', 'left_shoulder', 'left_elbow', 
                             'right_shoulder', 'right_elbow', 'left_hip', 'left_knee',
                             'right_hip', 'right_knee']

JOINTS_WITHOUT_ORIENTATION = ['left_hand', 'right_hand', 'left_foot', 'right_foot']

JOINTS = JOINTS_WITH_ORIENTATION + JOINTS_WITHOUT_ORIENTATION


#+end_src

**** Pose data structures
#+begin_src python
import numpy

class Joint:
  position = None;
  orientation = None;
    
  def __str__(self):
    return "Joint[\n Position: %s,\n Orientation:\n %s ]" % (self.position, self.orientation)
      

def parse_joint(data):
  joint = Joint();
  if len(data) > 4:
    joint.position = numpy.array(data[10:13]) / 1000;
    joint.orientation = numpy.array(data[0:9]).reshape((3,3));
  else:
    joint.position = numpy.array(data[0:3]) / 1000;
  return joint
  

class Pose:
  joints = dict();
   
  def __init__(self, data):
    pos = 1;

    for joint_name in JOINTS_WITH_ORIENTATION:
      joint = parse_joint(data[pos:pos+14]);
      pos += 14;
      self.joints[joint_name] = joint;

    for joint_name in JOINTS_WITHOUT_ORIENTATION:
      joint = parse_joint(data[pos:pos+4]);
      pos += 4;
      self.joints[joint_name]  = joint;
#+end_src

**** RVIZ visualization
***** Node setup
#+begin_src python
import roslib;
import rospy;
import math;
from visualization_msgs.msg import Marker
from visualization_msgs.msg import MarkerArray

topic = 'visualization_marker_array'
publisher = rospy.Publisher(topic, MarkerArray)

rospy.init_node('skeleton_pose_visualizer')

#+end_src

#+RESULTS:

***** ROS messages
#+begin_src python
def create_joint_message(joint, id=0):  
  marker = Marker()
  marker.header.frame_id = "/skeleton"
  marker.type = marker.SPHERE
  marker.id = id
  marker.action = marker.ADD
  marker.pose.position.x = joint.position[0]
  marker.pose.position.y = joint.position[1]
  marker.pose.position.z = joint.position[2]
  marker.scale.x = 0.05
  marker.scale.y = 0.05
  marker.scale.z = 0.05
  marker.color.a = 1.0
  marker.color.r = 1.0
  marker.color.g = 1.0
  marker.color.b = 0.0

  return marker

  
from geometry_msgs.msg import Point

def create_link_message(pose, id=0):

  def pos2Point(joint):
    return Point(joint.position[0], joint.position[1], joint.position[2]);

  points = []
  for jointName1 in LINKS.keys():
    for jointName2 in LINKS[jointName1]:
      joint1 = pose.joints[jointName1];
      joint2 = pose.joints[jointName2];
      points.append(pos2Point(joint1));
      points.append(pos2Point(joint2));

  marker = Marker()
  marker.header.frame_id = "/skeleton"
  marker.type = marker.LINE_LIST
  marker.id = id
  marker.action = marker.ADD
  marker.scale.x = 0.02
  marker.color.a = 1.0
  marker.color.r = 1.0
  marker.points = points

  return marker


  
def create_pose_message(pose):
  markerArray = MarkerArray()
  id = 0
  for joint in pose.joints.values():
    markerArray.markers.append(create_joint_message(joint, id))
    id += 1    
    markerArray.markers.append(create_link_message(pose, id))

  return markerArray

#+end_src

#+begin_src python
def visualize_frame(frame, dataset_person=DatasetPerson()):
  publisher.publish(create_pose_message(dataset_person.get_pose(frame)))


import time

def visualize_interval(start_frame=1, end_frame=1000, dataset_person=DatasetPerson()):
  for frame in range(start_frame, end_frame):
    visualize_frame(frame, dataset_person);
    time.sleep(1.0/25.0)
#+end_src

** gplvm
#+begin_src python
import numpy as np
import string
import matplotlib.pyplot as pb
import GPy

def learn_GPLVM(activity):
  p = DatasetPerson();
  p.load_activity(activity);
  data = p.get_processed_data();
  input_dim = 3
  kern = GPy.kern.rbf(input_dim)
  # kern = GPy.kern.periodic_exponential()
  m = GPy.models.BCGPLVM(data, input_dim=input_dim, kernel=kern)

  # initialize noise as 1% of variance in data
  # m['noise'] = m.likelihood.Y.var()/100.
  m.optimize('scg', messages=1, max_iters=1000)

  return m
#+end_src

#+begin_src python
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

def visualize_latent_model(model):
  fig = plt.figure()
  ax = fig.add_subplot(111, projection='3d')

  xs = model.X[:,0]
  ys = model.X[:,1]
  zs = model.X[:,2]
  ax.scatter(xs, ys, zs)

  ax.set_xlabel('latent 1')
  ax.set_ylabel('latent 2')
  ax.set_zlabel('latent 3')

  plt.show()

#+end_src

#+begin_src python
import GPy
#+end_src

** Sandbox
#+begin_src python

#+end_src

* Unsorted                                                         :noexport:
** Links
- [[http://glowingpython.blogspot.de/2012/10/visualizing-correlation-matrices.html][visualizing a correlation matrix]]
** Cites
*** Simplicity
Simplicity is a great virtue but it
requires hard work to achieve it
and education to appreciate it.
And to make matters worse:
complexity sells better.
Edsger Wybe Dijkstra 

Simplicity is the ultimate
sophistication.
Leonardo da Vinci
** Ideas
* Deprecated                                                       :noexport:
** Lisp
*** Configuration
**** Prerequisites
***** Common lisp
- sbcl
- quicklisp
***** System
- ros (hydro)
- gsl library

**** Start roscore
#+begin_src sh :results output :shebang "#!/bin/bash" :session test
 roscore&
#+end_src


**** Common Lisp Initialization
[[http://common-lisp.net/project/asdf/asdf/Configuring-ASDF.html][Configuring ASDF]]

Install all ros related packages. e.g:
#+begin_src sh
 sudo apt-get install ros-hydro-roslisp*
 sudo apt-get install ros-hydro-cl-*
#+end_src


We want to run common lisp ros code outside of catkin.
Add the following two files:

***** ~/.config/common-lisp/source-registry.conf.d/roslisp.conf
#+begin_src lisp
(:tree "/opt/ros/hydro/share/")
#+end_src

***** ~/.config/common-lisp/source-registry.conf.d/msgs.conf
#+begin_src lisp
(:tree "/opt/ros/hydro/share/common-lisp/ros/")
#+end_src

*** Visualization

**** Lisp
***** Common lisp packages Initialization
#+begin_src lisp :session 
  (ql:quickload "cl-ppcre")
  (ql:quickload "gsll")
  (ql:quickload "roslisp")
  (ql:quickload "alexandria")

#+end_src

#+RESULTS:
| alexandria |


#+begin_src lisp  :session :results silent
  ; making sure that roslisp is loaded
  (asdf:operate 'asdf:load-op :roslisp)

  ; making really sure that roslisp is loaded
  (ros-load:load-system :roslisp)
  (ros-load:load-system :cl-transforms)  
  (ros-load:load-system :visualization_msgs-msg)
#+end_src

***** Utils
****** Data set reading utils
#+begin_src lisp :session
  (defun read-file (path)
    (let ((lines (make-array 1 :fill-pointer 0)))
      (with-open-file (stream path)
        (do ((line (read-line stream nil)
                   (read-line stream nil)))
            ((null line))
          (vector-push-extend line lines)))
      lines))
#+end_src

#+RESULTS:
: READ-FILE


#+begin_src lisp :session
(defun read-frame (frame &optional (data *annotations*))
    (mapcar #'read-from-string  (cl-ppcre:split "," (aref data frame))))
#+end_src

#+RESULTS:
: READ-FRAME

****** List -> multidimensional array (matrix)
#+begin_src lisp :session
(defun list->matrix (lst)
           (let ((array (make-array '(3 3))))
             (setf (aref array 0 0) (first lst))
             (setf (aref array 0 1) (second lst))
             (setf (aref array 0 2) (third lst))
             (setf (aref array 1 0) (fourth lst))
             (setf (aref array 1 1) (fifth lst))
             (setf (aref array 1 2) (sixth lst))
             (setf (aref array 2 0) (seventh lst))
             (setf (aref array 2 1) (eighth lst))
             (setf (aref array 2 2) (ninth lst))
             array))
#+end_src

#+RESULTS:
: LIST->MATRIX

***** Data: Joint/Skeleton objects
 #+begin_src lisp  :session
   (defstruct joint
     position 
     orientation)
   
   (defstruct skeleton
     frame
     joints
     links)
   
   (defmacro x-pos (joint)
     `(first (joint-position ,joint)))
   
   (defmacro y-pos (joint)
     `(second (joint-position ,joint)))
   
   (defmacro z-pos (joint)
     `(third (joint-position ,joint)))
#+end_src

 #+RESULTS:
 : Z-POS

***** Function: Parse the data and create a skeleton object

#+begin_src lisp :session 
  
  (defvar *links*  '((torso neck) (torso left_shoulder) (torso right_shoulder)
                     (torso left_hip) (torso right_hip)  (neck head) 
                     (left_shoulder left_elbow) (right_shoulder right_elbow)
                     (right_elbow right_hand) (left_elbow left_hand)
                     (right_shoulder left_shoulder)
                     (left_hip left_knee) (right_hip right_knee)
                     (left_knee left_foot) (right_knee right_foot)
                     (left_hip right_hip)))
  
  (defvar *joints-with-orientation* '(head neck torso left_shoulder left_elbow 
                          right_shoulder right_elbow left_hip left_knee
                          right_hip right_knee))

  (defvar *joints-without-orientation* '(left_hand right_hand left_foot right_foot))

  (defvar *joints* (append *joints-with-orientation* *joints-without-orientation*))

#+end_src

#+RESULTS:
: *JOINTS*


#+begin_src lisp :session 
  (defun create-joint-from-list (lst)
    (make-joint
     :orientation (list->matrix (subseq lst 0 9))
     :position (subseq lst 10 14)))
  
  (defun create-skeleton-from-data (lst)
    (let ((start 0))
      (flet ((next-chunk (size)
               (let ((result (subseq lst start (+ start size))))
                 (setf start (+ start size ))
                 result)))
        (let ((frame (next-chunk 1))
              (joints nil)
              (links *links*))
          (dolist (joint-name *joints-with-orientation*)
            (push (cons joint-name (create-joint-from-list (next-chunk 14))) joints))
          
          (dolist (joint-name *joints-without-orientation*)
            (push (cons joint-name (make-joint :position (next-chunk 4))) joints))
          
          (make-skeleton :frame frame :joints joints :links links)))))  
#+end_src

#+RESULTS:
: CREATE-SKELETON-FROM-DATA

***** Function: create ros messages

#+begin_src lisp  :session
  (defun create-joint-message (joint id)
    (let ((pos (joint-position joint)))
      (roslisp:make-message 
       "visualization_msgs/Marker"
       (stamp header) (roslisp:ros-time)
       (frame_id header) "/skeleton" 
       (id) id
       (type)  (roslisp-msg-protocol:symbol-code
                'visualization_msgs-msg:<marker>
                :sphere)
       (action) (roslisp-msg-protocol:symbol-code
                 'visualization_msgs-msg:<marker>
                 :add)
       (x position pose) (/ (first pos) 1000)
       (y position pose) (/ (second pos) 1000)
       (z position pose) (/ (third pos) 1000)
       (x scale) 0.03
       (y scale) 0.03
       (z scale) 0.03
       (g color) 1.0
       (a color) 1.0
       (lifetime) 100)))
#+end_src

#+RESULTS:
: CREATE-JOINT-MESSAGE

#+begin_src lisp :session
  (defun create-link-list-message (points id)
    (roslisp:make-msg 
     "visualization_msgs/Marker"
     (stamp header) (roslisp:ros-time)
     (frame_id header) "/skeleton" (id) id
     (type)
     (roslisp-msg-protocol:symbol-code
      'visualization_msgs-msg:<marker>
      :line_list)
     (action)
     (roslisp-msg-protocol:symbol-code
      'visualization_msgs-msg:<marker>
      :add)
     (x scale) 0.01
     (r color) 1.0
     (a color) 1.0
     (lifetime) 100
     (points) points))
  
  (defun links->line-points (links joints)
    (let ((points nil))
      (mapcar 
       (lambda (el)
         (let ((p1 (joint-position (cdr (assoc (first el) joints))))
               (p2 (joint-position (cdr (assoc (second el) joints)))))
           (push (roslisp:make-msg "geometry_msgs/Point" 
                                   :x (/ (first p1) 1000)
                                   :y (/ (second p1) 1000)
                                   :z (/ (third p1) 1000)) points)
           (push (roslisp:make-msg "geometry_msgs/Point"
                                   :x (/ (first p2) 1000)
                                   :y (/ (second p2) 1000)
                                   :z (/ (third p2) 1000)) points))) 
       links)
      (map 'vector #'identity points)))
  
#+end_src

#+RESULTS:
: LINKS->LINE-POINTS

#+begin_src lisp :session
      (defun create-skeleton-message (skeleton)
        (let ((index 0) (markers 'nil))
          (mapcar (lambda (el) 
                    (push (create-joint-message (cdr el) index) markers)
                    (incf index))
                  (skeleton-joints skeleton))
          
          (push (create-link-list-message 
                 (links->line-points 
                  (skeleton-links skeleton) 
                  (skeleton-joints skeleton))
                 index) 
                markers)
          (roslisp:make-msg "visualization_msgs/MarkerArray" :markers
                            (map 'vector #'identity markers))))
#+end_src

#+RESULTS:
: CREATE-SKELETON-MESSAGE

***** Visualize a frame

#+begin_src lisp :session
  (defun visualize-frame (frame &optional (data *annotations*) (pub *pub*))
    (roslisp:publish pub 
                     (create-skeleton-message (create-skeleton-from-data (read-frame frame data)))))
#+end_src

#+RESULTS:
: VISUALIZE-FRAME

#+begin_src lisp :session
    (defun visualize-interval (start-frame end-frame &optional (data *annotations*) (pub *pub*) (sleep-time 0.05))
      (loop for frame from start-frame to end-frame do
        (progn
          (visualize-frame frame data pub)
          (sleep sleep-time))))
#+end_src

#+RESULTS:
: VISUALIZE-INTERVAL

**** Lisp: visualization test

#+begin_src lisp :session
  (ROSLISP:START-ROS-NODE "test")
  (defvar *pub* (ROSLISP:ADVERTISE "visualization_marker_array" "visualization_msgs/MarkerArray"))
  (defvar *annotations* (read-file "/work/Data/human_activities/data1/0512164529.txt"))

  (visualize-interval 1 1000)
#+end_src

#+RESULTS:
: NIL

* Footnotes

[fn:1] Human Activity Detection from RGBD Images, Jaeyong Sung, Colin Ponce, Bart Selman, Ashutosh Saxena. In AAAI workshop on Pattern, Activity and Intent Recognition (PAIR), 2011. 
[fn:2] RGB-D Camera-based Daily Living Activity Recognition - Chenyang Zhang, Student Member, IEEE and Yingli Tian, Senior Member, IEEE
 
